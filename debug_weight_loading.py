#!/usr/bin/env python3
"""
è°ƒè¯•æƒé‡åŠ è½½é—®é¢˜ - åˆ†æé¢„è®­ç»ƒæƒé‡ç»“æ„
"""

import torch
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

def analyze_pretrained_weights():
    """åˆ†æé¢„è®­ç»ƒæƒé‡æ–‡ä»¶ç»“æ„"""
    
    # é¢„è®­ç»ƒæƒé‡æ–‡ä»¶è·¯å¾„
    pretrained_path = '/dataNAS/people/joycewyr/Chest_CT_Cardiac/merlin/models/checkpoints/i3_resnet_clinical_longformer_best_clip_04-02-2024_23-21-36_epoch_99.pt'
    
    print("ğŸ” åˆ†æé¢„è®­ç»ƒæƒé‡æ–‡ä»¶ç»“æ„")
    print("=" * 80)
    
    try:
        # åŠ è½½æƒé‡æ–‡ä»¶
        print(f"ğŸ“ åŠ è½½æƒé‡æ–‡ä»¶: {pretrained_path}")
        state_dict = torch.load(pretrained_path, map_location='cpu')
        
        print(f"âœ… æƒé‡æ–‡ä»¶åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(state_dict)} ä¸ªå‚æ•°")
        
        # åˆ†ææƒé‡é”®ç»“æ„
        print("\nğŸ“Š æƒé‡é”®ç»“æ„åˆ†æ:")
        
        # ç»Ÿè®¡ä¸åŒå‰ç¼€çš„æƒé‡
        prefix_counts = {}
        encode_image_keys = []
        
        for key in state_dict.keys():
            if key.startswith('encode_image.'):
                encode_image_keys.append(key)
                
            # æå–å‰ç¼€
            if '.' in key:
                prefix = key.split('.')[0]
                prefix_counts[prefix] = prefix_counts.get(prefix, 0) + 1
        
        print(f"æƒé‡å‰ç¼€ç»Ÿè®¡:")
        for prefix, count in sorted(prefix_counts.items()):
            print(f"  {prefix}: {count} ä¸ªæƒé‡")
        
        # åˆ†æ encode_image ç›¸å…³çš„æƒé‡
        print(f"\nğŸ¯ encode_image ç›¸å…³æƒé‡ (å…± {len(encode_image_keys)} ä¸ª):")
        
        # æŒ‰å±‚åˆ†ç»„æ˜¾ç¤º
        layer_groups = {}
        for key in encode_image_keys:
            # ç§»é™¤ encode_image. å‰ç¼€
            clean_key = key.replace('encode_image.', '')
            
            # æå–å±‚å
            if clean_key.startswith('i3_resnet.'):
                layer_part = clean_key.replace('i3_resnet.', '')
                if '.' in layer_part:
                    layer_name = layer_part.split('.')[0]
                else:
                    layer_name = layer_part
                
                if layer_name not in layer_groups:
                    layer_groups[layer_name] = []
                layer_groups[layer_name].append(key)
        
        print("æŒ‰å±‚åˆ†ç»„çš„æƒé‡:")
        for layer_name, keys in sorted(layer_groups.items()):
            print(f"  {layer_name}: {len(keys)} ä¸ªæƒé‡")
            if len(keys) <= 10:  # å¦‚æœæƒé‡å°‘äº10ä¸ªï¼Œæ˜¾ç¤ºå…¨éƒ¨
                for key in keys:
                    print(f"    - {key}")
            else:  # å¦åˆ™æ˜¾ç¤ºå‰5ä¸ªå’Œå5ä¸ª
                for key in keys[:5]:
                    print(f"    - {key}")
                print(f"    ... ({len(keys) - 10} ä¸ªæƒé‡çœç•¥)")
                for key in keys[-5:]:
                    print(f"    - {key}")
        
        # æ£€æŸ¥æ—©æœŸå±‚æƒé‡
        print(f"\nğŸ” æ—©æœŸå±‚æƒé‡æ£€æŸ¥:")
        early_layers = ['conv1', 'bn1', 'layer1', 'layer2']
        
        for layer in early_layers:
            found_keys = [key for key in encode_image_keys if f'i3_resnet.{layer}' in key]
            if found_keys:
                print(f"  âœ… {layer}: æ‰¾åˆ° {len(found_keys)} ä¸ªæƒé‡")
                for key in found_keys[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                    print(f"    - {key}")
                if len(found_keys) > 3:
                    print(f"    ... è¿˜æœ‰ {len(found_keys) - 3} ä¸ª")
            else:
                print(f"  âŒ {layer}: æœªæ‰¾åˆ°æƒé‡")
        
        # æ£€æŸ¥åæœŸå±‚æƒé‡
        print(f"\nğŸ” åæœŸå±‚æƒé‡æ£€æŸ¥:")
        late_layers = ['layer3', 'layer4', 'classifier', 'contrastive_head']
        
        for layer in late_layers:
            found_keys = [key for key in encode_image_keys if f'i3_resnet.{layer}' in key]
            if found_keys:
                print(f"  âœ… {layer}: æ‰¾åˆ° {len(found_keys)} ä¸ªæƒé‡")
                for key in found_keys[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                    print(f"    - {key}")
                if len(found_keys) > 3:
                    print(f"    ... è¿˜æœ‰ {len(found_keys) - 3} ä¸ª")
            else:
                print(f"  âŒ {layer}: æœªæ‰¾åˆ°æƒé‡")
        
        # åˆ†ææƒé‡æ˜ å°„
        print(f"\nğŸ”§ æƒé‡æ˜ å°„åˆ†æ:")
        
        # æ¨¡æ‹Ÿæƒé‡æ˜ å°„è¿‡ç¨‹
        image_encoder_dict = {}
        for key, value in state_dict.items():
            if key.startswith('encode_image.i3_resnet.'):
                new_key = key.replace('encode_image.', '')
                image_encoder_dict[new_key] = value
        
        print(f"æˆåŠŸæ˜ å°„çš„æƒé‡æ•°é‡: {len(image_encoder_dict)}")
        
        # åˆ†ææ˜ å°„åçš„æƒé‡ç»“æ„
        mapped_layer_groups = {}
        for key in image_encoder_dict.keys():
            layer_part = key.replace('i3_resnet.', '')
            if '.' in layer_part:
                layer_name = layer_part.split('.')[0]
            else:
                layer_name = layer_part
            
            if layer_name not in mapped_layer_groups:
                mapped_layer_groups[layer_name] = []
            mapped_layer_groups[layer_name].append(key)
        
        print("æ˜ å°„åçš„å±‚æƒé‡åˆ†å¸ƒ:")
        for layer_name, keys in sorted(mapped_layer_groups.items()):
            print(f"  {layer_name}: {len(keys)} ä¸ªæƒé‡")
        
        # æ£€æŸ¥æ‰€æœ‰é”®
        print(f"\nğŸ“‹ æ‰€æœ‰æƒé‡é”®é¢„è§ˆ (å‰20ä¸ª):")
        for i, key in enumerate(sorted(state_dict.keys())[:20]):
            print(f"  {i+1:2d}. {key}")
        
        print(f"\nğŸ“‹ æ‰€æœ‰æƒé‡é”®é¢„è§ˆ (å20ä¸ª):")
        for i, key in enumerate(sorted(state_dict.keys())[-20:]):
            print(f"  {len(state_dict) - 20 + i + 1:2d}. {key}")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    analyze_pretrained_weights() 