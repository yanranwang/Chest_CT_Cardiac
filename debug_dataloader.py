#!/usr/bin/env python3
"""
æ•°æ®åŠ è½½å™¨debugè„šæœ¬
ç”¨äºé€æ­¥æ£€æŸ¥æ•°æ®åŠ è½½å™¨åˆ›å»ºè¿‡ç¨‹ä¸­çš„é—®é¢˜
"""

import os
import sys
import json
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

def check_config_file():
    """æ£€æŸ¥é…ç½®æ–‡ä»¶"""
    print("=" * 60)
    print("ğŸ” æ­¥éª¤1: æ£€æŸ¥é…ç½®æ–‡ä»¶")
    print("=" * 60)
    
    config_path = "configs/fast_training_config.json"
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        print(f"âœ… é…ç½®æ–‡ä»¶è¯»å–æˆåŠŸ: {config_path}")
        print(f"   å¿«é€Ÿæ•°æ®åŠ è½½å™¨: {config.get('use_fast_loader', False)}")
        print(f"   é¢„å¤„ç†æ•°æ®ç›®å½•: {config.get('preprocessed_data_dir')}")
        print(f"   æ‰¹æ¬¡å¤§å°: {config.get('batch_size')}")
        print(f"   å·¥ä½œè¿›ç¨‹æ•°: {config.get('num_workers')}")
        print(f"   é¢„åŠ è½½è®­ç»ƒæ•°æ®: {config.get('cache_config', {}).get('preload_train_data', False)}")
        print(f"   é¢„åŠ è½½éªŒè¯æ•°æ®: {config.get('cache_config', {}).get('preload_val_data', False)}")
        print(f"   ç¼“å­˜å¤§å°: {config.get('cache_config', {}).get('cache_size', 1000)}")
        
        return config
        
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        return None

def check_preprocessed_data(config):
    """æ£€æŸ¥é¢„å¤„ç†æ•°æ®"""
    print("\n" + "=" * 60)
    print("ğŸ” æ­¥éª¤2: æ£€æŸ¥é¢„å¤„ç†æ•°æ®")
    print("=" * 60)
    
    preprocessed_dir = config.get('preprocessed_data_dir')
    if not preprocessed_dir:
        print("âŒ æœªæŒ‡å®šé¢„å¤„ç†æ•°æ®ç›®å½•")
        return False
    
    hdf5_path = Path(preprocessed_dir) / 'preprocessed_data.h5'
    metadata_path = Path(preprocessed_dir) / 'data_metadata.json'
    
    print(f"é¢„å¤„ç†æ•°æ®ç›®å½•: {preprocessed_dir}")
    print(f"HDF5æ–‡ä»¶: {hdf5_path}")
    print(f"å…ƒæ•°æ®æ–‡ä»¶: {metadata_path}")
    
    if not hdf5_path.exists():
        print(f"âŒ HDF5æ–‡ä»¶ä¸å­˜åœ¨: {hdf5_path}")
        return False
    
    if not metadata_path.exists():
        print(f"âŒ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_path}")
        return False
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    hdf5_size = hdf5_path.stat().st_size / (1024*1024)  # MB
    metadata_size = metadata_path.stat().st_size / 1024  # KB
    
    print(f"âœ… HDF5æ–‡ä»¶å¤§å°: {hdf5_size:.2f} MB")
    print(f"âœ… å…ƒæ•°æ®æ–‡ä»¶å¤§å°: {metadata_size:.2f} KB")
    
    return True

def check_metadata_loading(config):
    """æ£€æŸ¥å…ƒæ•°æ®åŠ è½½"""
    print("\n" + "=" * 60)
    print("ğŸ” æ­¥éª¤3: æ£€æŸ¥å…ƒæ•°æ®åŠ è½½")
    print("=" * 60)
    
    try:
        metadata_path = Path(config['preprocessed_data_dir']) / 'data_metadata.json'
        
        print("æ­£åœ¨åŠ è½½å…ƒæ•°æ®...")
        start_time = time.time()
        
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        load_time = time.time() - start_time
        
        print(f"âœ… å…ƒæ•°æ®åŠ è½½æˆåŠŸï¼Œè€—æ—¶: {load_time:.2f} ç§’")
        print(f"   æ•°æ®é¡¹æ•°é‡: {len(metadata.get('items', {}))}")
        print(f"   æ‚£è€…æ•°é‡: {len(metadata.get('patient_mapping', {}))}")
        print(f"   æ–‡ä»¶å¤¹æ•°é‡: {len(metadata.get('folder_mapping', {}))}")
        
        return metadata
        
    except Exception as e:
        print(f"âŒ å…ƒæ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

def check_data_splitting(config, metadata):
    """æ£€æŸ¥æ•°æ®åˆ†å‰²"""
    print("\n" + "=" * 60)
    print("ğŸ” æ­¥éª¤4: æ£€æŸ¥æ•°æ®åˆ†å‰²")
    print("=" * 60)
    
    try:
        from sklearn.model_selection import train_test_split
        
        all_item_ids = list(metadata['items'].keys())
        print(f"æ€»æ•°æ®é¡¹: {len(all_item_ids)}")
        
        start_time = time.time()
        
        # æ‰§è¡Œæ•°æ®åˆ†å‰²
        train_ids, val_ids = train_test_split(
            all_item_ids,
            train_size=config.get('train_val_split', 0.8),
            random_state=config.get('seed', 42),
            shuffle=True
        )
        
        split_time = time.time() - start_time
        
        print(f"âœ… æ•°æ®åˆ†å‰²å®Œæˆï¼Œè€—æ—¶: {split_time:.2f} ç§’")
        print(f"   è®­ç»ƒé›†: {len(train_ids)} é¡¹")
        print(f"   éªŒè¯é›†: {len(val_ids)} é¡¹")
        
        return train_ids, val_ids
        
    except Exception as e:
        print(f"âŒ æ•°æ®åˆ†å‰²å¤±è´¥: {e}")
        return None, None

def check_hdf5_access(config):
    """æ£€æŸ¥HDF5æ–‡ä»¶è®¿é—®"""
    print("\n" + "=" * 60)
    print("ğŸ” æ­¥éª¤5: æ£€æŸ¥HDF5æ–‡ä»¶è®¿é—®")
    print("=" * 60)
    
    try:
        import h5py
        
        hdf5_path = Path(config['preprocessed_data_dir']) / 'preprocessed_data.h5'
        
        print("æ­£åœ¨æ‰“å¼€HDF5æ–‡ä»¶...")
        start_time = time.time()
        
        with h5py.File(hdf5_path, 'r') as f:
            open_time = time.time() - start_time
            
            print(f"âœ… HDF5æ–‡ä»¶æ‰“å¼€æˆåŠŸï¼Œè€—æ—¶: {open_time:.2f} ç§’")
            print(f"   æ ¹ç»„: {list(f.keys())}")
            
            if 'images' in f:
                images_group = f['images']
                print(f"   å›¾åƒç»„å¤§å°: {len(images_group)} é¡¹")
                
                # æµ‹è¯•è¯»å–ç¬¬ä¸€ä¸ªæ•°æ®é¡¹
                if len(images_group) > 0:
                    first_key = list(images_group.keys())[0]
                    print(f"   æµ‹è¯•è¯»å–ç¬¬ä¸€ä¸ªæ•°æ®é¡¹: {first_key}")
                    
                    start_time = time.time()
                    test_data = images_group[first_key][:]
                    read_time = time.time() - start_time
                    
                    print(f"   âœ… æ•°æ®è¯»å–æˆåŠŸï¼Œè€—æ—¶: {read_time:.2f} ç§’")
                    print(f"   æ•°æ®å½¢çŠ¶: {test_data.shape}")
                    print(f"   æ•°æ®ç±»å‹: {test_data.dtype}")
        
        return True
        
    except Exception as e:
        print(f"âŒ HDF5æ–‡ä»¶è®¿é—®å¤±è´¥: {e}")
        return False

def check_dataset_creation(config, train_ids, val_ids):
    """æ£€æŸ¥æ•°æ®é›†åˆ›å»º"""
    print("\n" + "=" * 60)
    print("ğŸ” æ­¥éª¤6: æ£€æŸ¥æ•°æ®é›†åˆ›å»º")
    print("=" * 60)
    
    try:
        from merlin.training.fast_dataloader import FastCardiacDataset
        
        hdf5_path = Path(config['preprocessed_data_dir']) / 'preprocessed_data.h5'
        metadata_path = Path(config['preprocessed_data_dir']) / 'data_metadata.json'
        
        # ä½¿ç”¨è¾ƒå°çš„æ ·æœ¬è¿›è¡Œæµ‹è¯•
        test_train_ids = train_ids[:10] if len(train_ids) > 10 else train_ids
        test_val_ids = val_ids[:5] if len(val_ids) > 5 else val_ids
        
        print(f"æµ‹è¯•è®­ç»ƒé›†åˆ›å»ºï¼ˆ{len(test_train_ids)} é¡¹ï¼‰...")
        start_time = time.time()
        
        # åˆ›å»ºè®­ç»ƒæ•°æ®é›†ï¼Œä¸é¢„åŠ è½½æ•°æ®
        train_dataset = FastCardiacDataset(
            hdf5_path=str(hdf5_path),
            metadata_path=str(metadata_path),
            item_ids=test_train_ids,
            enable_cache=False,  # å…³é—­ç¼“å­˜
            preload_data=False   # ä¸é¢„åŠ è½½æ•°æ®
        )
        
        creation_time = time.time() - start_time
        
        print(f"âœ… è®­ç»ƒæ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œè€—æ—¶: {creation_time:.2f} ç§’")
        print(f"   æ•°æ®é›†å¤§å°: {len(train_dataset)}")
        
        # æµ‹è¯•æ•°æ®è®¿é—®
        print("æµ‹è¯•æ•°æ®è®¿é—®...")
        start_time = time.time()
        
        sample = train_dataset[0]
        
        access_time = time.time() - start_time
        
        print(f"âœ… æ•°æ®è®¿é—®æˆåŠŸï¼Œè€—æ—¶: {access_time:.2f} ç§’")
        print(f"   å›¾åƒå½¢çŠ¶: {sample['image'].shape}")
        print(f"   å¿ƒè„åŠŸèƒ½æŒ‡æ ‡: {sample['cardiac_metrics']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
        return False

def check_dataloader_creation(config, train_ids, val_ids):
    """æ£€æŸ¥æ•°æ®åŠ è½½å™¨åˆ›å»º"""
    print("\n" + "=" * 60)
    print("ğŸ” æ­¥éª¤7: æ£€æŸ¥æ•°æ®åŠ è½½å™¨åˆ›å»º")
    print("=" * 60)
    
    try:
        from merlin.training.fast_dataloader import FastCardiacDataset
        from torch.utils.data import DataLoader
        
        hdf5_path = Path(config['preprocessed_data_dir']) / 'preprocessed_data.h5'
        metadata_path = Path(config['preprocessed_data_dir']) / 'data_metadata.json'
        
        # ä½¿ç”¨è¾ƒå°çš„æ ·æœ¬è¿›è¡Œæµ‹è¯•
        test_train_ids = train_ids[:10] if len(train_ids) > 10 else train_ids
        
        print(f"åˆ›å»ºæ•°æ®é›†ï¼ˆ{len(test_train_ids)} é¡¹ï¼‰...")
        train_dataset = FastCardiacDataset(
            hdf5_path=str(hdf5_path),
            metadata_path=str(metadata_path),
            item_ids=test_train_ids,
            enable_cache=False,
            preload_data=False
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼Œä½¿ç”¨è¾ƒå°‘çš„å·¥ä½œè¿›ç¨‹
        print("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        start_time = time.time()
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=2,  # å°æ‰¹æ¬¡
            shuffle=True,
            num_workers=0,  # ä¸ä½¿ç”¨å¤šè¿›ç¨‹
            pin_memory=False,
            drop_last=True
        )
        
        creation_time = time.time() - start_time
        
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸï¼Œè€—æ—¶: {creation_time:.2f} ç§’")
        
        # æµ‹è¯•æ•°æ®è¿­ä»£
        print("æµ‹è¯•æ•°æ®è¿­ä»£...")
        start_time = time.time()
        
        batch = next(iter(train_loader))
        
        iter_time = time.time() - start_time
        
        print(f"âœ… æ•°æ®è¿­ä»£æˆåŠŸï¼Œè€—æ—¶: {iter_time:.2f} ç§’")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch['image'].shape[0]}")
        print(f"   å›¾åƒå½¢çŠ¶: {batch['image'].shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ•°æ®åŠ è½½å™¨debug")
    print("=" * 80)
    
    # æ­¥éª¤1: æ£€æŸ¥é…ç½®æ–‡ä»¶
    config = check_config_file()
    if not config:
        return
    
    # æ­¥éª¤2: æ£€æŸ¥é¢„å¤„ç†æ•°æ®
    if not check_preprocessed_data(config):
        return
    
    # æ­¥éª¤3: æ£€æŸ¥å…ƒæ•°æ®åŠ è½½
    metadata = check_metadata_loading(config)
    if not metadata:
        return
    
    # æ­¥éª¤4: æ£€æŸ¥æ•°æ®åˆ†å‰²
    train_ids, val_ids = check_data_splitting(config, metadata)
    if not train_ids or not val_ids:
        return
    
    # æ­¥éª¤5: æ£€æŸ¥HDF5æ–‡ä»¶è®¿é—®
    if not check_hdf5_access(config):
        return
    
    # æ­¥éª¤6: æ£€æŸ¥æ•°æ®é›†åˆ›å»º
    if not check_dataset_creation(config, train_ids, val_ids):
        return
    
    # æ­¥éª¤7: æ£€æŸ¥æ•°æ®åŠ è½½å™¨åˆ›å»º
    if not check_dataloader_creation(config, train_ids, val_ids):
        return
    
    print("\n" + "=" * 80)
    print("ğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼")
    print("=" * 80)
    
    print("\nğŸ’¡ å»ºè®®çš„è§£å†³æ–¹æ¡ˆ:")
    print("1. å¦‚æœåœ¨æ­¥éª¤6æˆ–7å¤±è´¥ï¼Œå¯èƒ½æ˜¯é…ç½®é—®é¢˜")
    print("2. å°è¯•å‡å°‘num_workersçš„å€¼")
    print("3. è®¾ç½®preload_train_dataå’Œpreload_val_dataä¸ºfalse")
    print("4. å‡å°‘cache_sizeçš„å€¼")
    print("5. å‡å°‘batch_sizeçš„å€¼")

if __name__ == '__main__':
    main() 