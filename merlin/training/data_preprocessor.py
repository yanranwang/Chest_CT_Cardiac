#!/usr/bin/env python3
"""
æ•°æ®é¢„å¤„ç†å™¨ - æ‰¹é‡é¢„å¤„ç†å¿ƒè„åŠŸèƒ½è®­ç»ƒæ•°æ®

è¯¥è„šæœ¬ç”¨äºæ‰¹é‡é¢„å¤„ç†æ‰€æœ‰è®­ç»ƒæ•°æ®ï¼Œå°†å¤„ç†åçš„æ•°æ®ä¿å­˜åˆ°HDF5æ–‡ä»¶ä¸­ï¼Œ
ä»¥åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹å¹¶æé«˜GPUåˆ©ç”¨ç‡ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. æ‰¹é‡å¤„ç†æ‰€æœ‰å›¾åƒæ•°æ®
2. ä¿å­˜åˆ°HDF5æ•°æ®åº“
3. ç”Ÿæˆæ•°æ®ç´¢å¼•
4. éªŒè¯æ•°æ®å®Œæ•´æ€§
5. æ”¯æŒå¢é‡æ›´æ–°

ä½¿ç”¨æ–¹æ³•:
    python data_preprocessor.py --config config.json
"""

import os
import h5py
import json
import numpy as np
import pandas as pd
import torch
from pathlib import Path
from tqdm import tqdm
import hashlib
import multiprocessing as mp
from functools import partial
import logging
from typing import Dict, List, Any, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
import sys
sys.path.append(str(Path(__file__).parent.parent))

from merlin.data.monai_transforms import ImageTransforms
from merlin.training.cardiac_trainer import load_and_validate_csv_data, build_data_list


class DataPreprocessor:
    """æ•°æ®é¢„å¤„ç†å™¨ - æ‰¹é‡é¢„å¤„ç†å¹¶ä¿å­˜åˆ°HDF5"""
    
    def __init__(self, config: dict):
        self.config = config
        self.setup_logging()
        
        # è¾“å‡ºè·¯å¾„
        self.output_dir = Path(config['output_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # HDF5æ–‡ä»¶è·¯å¾„
        self.hdf5_path = self.output_dir / 'preprocessed_data.h5'
        self.metadata_path = self.output_dir / 'data_metadata.json'
        
        # é¢„å¤„ç†è½¬æ¢
        self.transform = ImageTransforms
        
        # æ•°æ®ç»Ÿè®¡
        self.stats = {
            'total_files': 0,
            'processed_files': 0,
            'failed_files': 0,
            'skipped_files': 0,
            'processing_time': 0
        }
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = Path(self.config['output_dir']) / 'preprocessing.log'
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def get_data_hash(self, data_item: dict) -> str:
        """ç”Ÿæˆæ•°æ®é¡¹çš„å”¯ä¸€å“ˆå¸Œå€¼"""
        hash_content = f"{data_item['image']}_{data_item['basename']}_{data_item['folder']}"
        return hashlib.md5(hash_content.encode()).hexdigest()
    
    def preprocess_single_item(self, data_item: dict) -> Optional[Tuple[str, dict]]:
        """é¢„å¤„ç†å•ä¸ªæ•°æ®é¡¹"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            image_path = data_item.get('image', '')
            if not os.path.exists(image_path):
                self.logger.warning(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                return None
            
            # ç”Ÿæˆå”¯ä¸€ID
            item_id = self.get_data_hash(data_item)
            
            # åº”ç”¨å˜æ¢
            processed_item = self.transform(data_item)
            
            # æå–å›¾åƒæ•°æ®
            image_tensor = processed_item['image']
            
            # å‡†å¤‡å­˜å‚¨çš„æ•°æ®
            storage_data = {
                'image': image_tensor.numpy(),
                'cardiac_metrics': data_item.get('cardiac_metrics'),
                'patient_id': data_item.get('patient_id'),
                'basename': data_item.get('basename'),
                'folder': data_item.get('folder'),
                'metadata': data_item.get('metadata', {}),
                'original_path': data_item.get('image'),
                'processed_shape': image_tensor.shape,
                'data_type': str(image_tensor.dtype)
            }
            
            return item_id, storage_data
            
        except Exception as e:
            self.logger.error(f"é¢„å¤„ç†å¤±è´¥: {data_item.get('image', 'unknown')}, é”™è¯¯: {e}")
            return None
    
    def process_batch(self, data_batch: List[dict]) -> List[Tuple[str, dict]]:
        """æ‰¹é‡å¤„ç†æ•°æ®"""
        results = []
        
        for data_item in data_batch:
            result = self.preprocess_single_item(data_item)
            if result:
                results.append(result)
                self.stats['processed_files'] += 1
            else:
                self.stats['failed_files'] += 1
        
        return results
    
    def save_to_hdf5(self, processed_data: List[Tuple[str, dict]]):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®åˆ°HDF5æ–‡ä»¶"""
        self.logger.info(f"ä¿å­˜ {len(processed_data)} ä¸ªå¤„ç†åçš„æ•°æ®é¡¹åˆ° {self.hdf5_path}")
        
        with h5py.File(self.hdf5_path, 'a') as f:
            # åˆ›å»ºç»„
            if 'images' not in f:
                f.create_group('images')
            if 'metadata' not in f:
                f.create_group('metadata')
            
            for item_id, data in tqdm(processed_data, desc="ä¿å­˜åˆ°HDF5"):
                # ä¿å­˜å›¾åƒæ•°æ®
                if item_id not in f['images']:
                    f['images'].create_dataset(
                        item_id, 
                        data=data['image'],
                        compression='gzip',
                        compression_opts=9
                    )
                
                # ä¿å­˜å…ƒæ•°æ®
                if item_id not in f['metadata']:
                    # å°†å…ƒæ•°æ®è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ä¿å­˜
                    metadata_json = json.dumps({
                        'cardiac_metrics': data['cardiac_metrics'].tolist() if data['cardiac_metrics'] is not None else None,
                        'patient_id': data['patient_id'],
                        'basename': data['basename'],
                        'folder': data['folder'],
                        'metadata': data['metadata'],
                        'original_path': data['original_path'],
                        'processed_shape': data['processed_shape'],
                        'data_type': data['data_type']
                    }, ensure_ascii=False)
                    
                    f['metadata'].create_dataset(
                        item_id,
                        data=metadata_json,
                        dtype=h5py.string_dtype(encoding='utf-8')
                    )
    
    def create_index(self, data_list: List[dict]) -> Dict[str, Any]:
        """åˆ›å»ºæ•°æ®ç´¢å¼•"""
        index = {
            'items': {},
            'patient_mapping': {},
            'folder_mapping': {},
            'statistics': {
                'total_items': len(data_list),
                'unique_patients': len(set(item['patient_id'] for item in data_list)),
                'unique_folders': len(set(item['folder'] for item in data_list))
            }
        }
        
        for data_item in data_list:
            item_id = self.get_data_hash(data_item)
            
            # åŸºæœ¬ä¿¡æ¯
            index['items'][item_id] = {
                'patient_id': data_item['patient_id'],
                'basename': data_item['basename'],
                'folder': data_item['folder'],
                'original_path': data_item['image'],
                'has_cardiac_metrics': data_item.get('cardiac_metrics') is not None
            }
            
            # æ‚£è€…æ˜ å°„
            patient_id = data_item['patient_id']
            if patient_id not in index['patient_mapping']:
                index['patient_mapping'][patient_id] = []
            index['patient_mapping'][patient_id].append(item_id)
            
            # æ–‡ä»¶å¤¹æ˜ å°„
            folder = data_item['folder']
            if folder not in index['folder_mapping']:
                index['folder_mapping'][folder] = []
            index['folder_mapping'][folder].append(item_id)
        
        return index
    
    def verify_data_integrity(self) -> bool:
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        self.logger.info("éªŒè¯æ•°æ®å®Œæ•´æ€§...")
        
        try:
            with h5py.File(self.hdf5_path, 'r') as f:
                images_group = f['images']
                metadata_group = f['metadata']
                
                # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
                image_keys = set(images_group.keys())
                metadata_keys = set(metadata_group.keys())
                
                if image_keys != metadata_keys:
                    self.logger.error("å›¾åƒæ•°æ®å’Œå…ƒæ•°æ®é”®ä¸åŒ¹é…")
                    return False
                
                # éšæœºéªŒè¯å‡ ä¸ªæ ·æœ¬
                sample_keys = list(image_keys)[:min(10, len(image_keys))]
                
                for key in sample_keys:
                    # æ£€æŸ¥å›¾åƒæ•°æ®
                    image_data = images_group[key][:]
                    if image_data.size == 0:
                        self.logger.error(f"å›¾åƒæ•°æ®ä¸ºç©º: {key}")
                        return False
                    
                    # æ£€æŸ¥å…ƒæ•°æ®
                    metadata_str = metadata_group[key][()]
                    if isinstance(metadata_str, bytes):
                        metadata_str = metadata_str.decode('utf-8')
                    
                    try:
                        metadata = json.loads(metadata_str)
                    except json.JSONDecodeError:
                        self.logger.error(f"å…ƒæ•°æ®æ ¼å¼é”™è¯¯: {key}")
                        return False
                
                self.logger.info(f"æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡ï¼Œå…± {len(image_keys)} ä¸ªæ ·æœ¬")
                return True
                
        except Exception as e:
            self.logger.error(f"æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥: {e}")
            return False
    
    def get_existing_items(self) -> set:
        """è·å–å·²å­˜åœ¨çš„æ•°æ®é¡¹"""
        if not self.hdf5_path.exists():
            return set()
        
        try:
            with h5py.File(self.hdf5_path, 'r') as f:
                if 'images' in f:
                    return set(f['images'].keys())
                return set()
        except Exception as e:
            self.logger.warning(f"è¯»å–ç°æœ‰æ•°æ®å¤±è´¥: {e}")
            return set()
    
    def preprocess_data(self, data_list: List[dict], force_reprocess: bool = False):
        """é¢„å¤„ç†æ•°æ®çš„ä¸»å‡½æ•°"""
        import time
        start_time = time.time()
        
        self.logger.info(f"å¼€å§‹é¢„å¤„ç† {len(data_list)} ä¸ªæ•°æ®é¡¹")
        self.stats['total_files'] = len(data_list)
        
        # æ£€æŸ¥å·²å­˜åœ¨çš„æ•°æ®
        existing_items = set() if force_reprocess else self.get_existing_items()
        
        # è¿‡æ»¤éœ€è¦å¤„ç†çš„æ•°æ®
        items_to_process = []
        for data_item in data_list:
            item_id = self.get_data_hash(data_item)
            if item_id not in existing_items:
                items_to_process.append(data_item)
            else:
                self.stats['skipped_files'] += 1
        
        if not items_to_process:
            self.logger.info("æ‰€æœ‰æ•°æ®å·²ç»é¢„å¤„ç†å®Œæˆ")
            return
        
        self.logger.info(f"éœ€è¦å¤„ç† {len(items_to_process)} ä¸ªæ–°æ•°æ®é¡¹")
        
        # æ‰¹é‡å¤„ç†é…ç½®
        batch_size = self.config.get('preprocess_batch_size', 16)
        num_workers = self.config.get('num_workers', 1)  # é»˜è®¤ä½¿ç”¨å•è¿›ç¨‹
        enable_multiprocessing = self.config.get('enable_multiprocessing', False)
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¤šè¿›ç¨‹
        if enable_multiprocessing and num_workers > 1:
            self.logger.info(f"ä½¿ç”¨ {num_workers} ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†")
            try:
                self._multiprocess_preprocess(items_to_process, batch_size, num_workers)
            except Exception as e:
                self.logger.error(f"å¤šè¿›ç¨‹å¤„ç†å¤±è´¥: {e}")
                self.logger.info("å›é€€åˆ°å•è¿›ç¨‹å¤„ç†...")
                self._single_process_preprocess(items_to_process, batch_size)
        else:
            self.logger.info("ä½¿ç”¨å•è¿›ç¨‹å¤„ç†")
            self._single_process_preprocess(items_to_process, batch_size)
        
        # åˆ›å»ºç´¢å¼•
        self.logger.info("åˆ›å»ºæ•°æ®ç´¢å¼•...")
        index = self.create_index(data_list)
        
        # ä¿å­˜ç´¢å¼•
        with open(self.metadata_path, 'w', encoding='utf-8') as f:
            json.dump(index, f, indent=2, ensure_ascii=False)
        
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        if not self.verify_data_integrity():
            raise RuntimeError("æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥")
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats['processing_time'] = time.time() - start_time
        self._print_statistics()
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        with open(self.output_dir / 'preprocessing_stats.json', 'w') as f:
            json.dump(self.stats, f, indent=2)
    
    def _single_process_preprocess(self, items_to_process: List[dict], batch_size: int):
        """å•è¿›ç¨‹é¢„å¤„ç†"""
        total_batches = (len(items_to_process) + batch_size - 1) // batch_size
        
        for i in tqdm(range(0, len(items_to_process), batch_size), 
                      desc="é¢„å¤„ç†æ‰¹æ¬¡", 
                      total=total_batches,
                      ncols=100):
            batch = items_to_process[i:i + batch_size]
            processed_batch = self.process_batch(batch)
            
            if processed_batch:
                self.save_to_hdf5(processed_batch)
                self.logger.info(f"å·²å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{total_batches}, æˆåŠŸå¤„ç† {len(processed_batch)} ä¸ªæ•°æ®é¡¹")
            else:
                self.logger.warning(f"æ‰¹æ¬¡ {i//batch_size + 1}/{total_batches} å¤„ç†å¤±è´¥æˆ–æ— æœ‰æ•ˆæ•°æ®")
    
    def _multiprocess_preprocess(self, items_to_process: List[dict], batch_size: int, num_workers: int):
        """å¤šè¿›ç¨‹é¢„å¤„ç†"""
        # åˆ›å»ºæ•°æ®æ‰¹æ¬¡
        batches = [items_to_process[i:i + batch_size] for i in range(0, len(items_to_process), batch_size)]
        
        # ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†
        with mp.Pool(processes=num_workers) as pool:
            # ä½¿ç”¨ç‹¬ç«‹çš„å¤„ç†å‡½æ•°é¿å…åºåˆ—åŒ–é—®é¢˜
            results = pool.map(process_batch_standalone, batches)
        
        # ä¿å­˜ç»“æœ
        for processed_batch in results:
            if processed_batch:
                self.save_to_hdf5(processed_batch)
    
    def _print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        self.logger.info("=" * 80)
        self.logger.info("ğŸ“Š é¢„å¤„ç†ç»Ÿè®¡ä¿¡æ¯")
        self.logger.info("=" * 80)
        self.logger.info(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}")
        self.logger.info(f"âœ… æˆåŠŸå¤„ç†: {self.stats['processed_files']}")
        self.logger.info(f"â­ï¸  è·³è¿‡æ–‡ä»¶: {self.stats['skipped_files']}")
        self.logger.info(f"âŒ å¤±è´¥æ–‡ä»¶: {self.stats['failed_files']}")
        self.logger.info(f"â±ï¸  å¤„ç†æ—¶é—´: {self.stats['processing_time']:.2f} ç§’")
        if self.stats['total_files'] > 0:
            success_rate = (self.stats['processed_files'] / self.stats['total_files']) * 100
            self.logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.2f}%")
        self.logger.info(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {self.hdf5_path}")
        self.logger.info(f"ğŸ“‹ ç´¢å¼•æ–‡ä»¶: {self.metadata_path}")
        self.logger.info("=" * 80)


def process_batch_standalone(data_batch: List[dict]) -> List[Tuple[str, dict]]:
    """ç‹¬ç«‹çš„æ‰¹å¤„ç†å‡½æ•°ï¼Œç”¨äºå¤šè¿›ç¨‹å¤„ç†"""
    results = []
    
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    from merlin.data.monai_transforms import ImageTransforms
    import hashlib
    
    transform = ImageTransforms
    
    for data_item in data_batch:
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            image_path = data_item.get('image', '')
            if not os.path.exists(image_path):
                print(f"è­¦å‘Š: å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                continue
            
            # ç”Ÿæˆå”¯ä¸€ID
            hash_content = f"{data_item['image']}_{data_item['basename']}_{data_item['folder']}"
            item_id = hashlib.md5(hash_content.encode()).hexdigest()
            
            # åº”ç”¨å˜æ¢
            processed_item = transform(data_item)
            
            # æå–å›¾åƒæ•°æ®
            image_tensor = processed_item['image']
            
            # å‡†å¤‡å­˜å‚¨çš„æ•°æ®
            storage_data = {
                'image': image_tensor.numpy(),
                'cardiac_metrics': data_item.get('cardiac_metrics'),
                'patient_id': data_item.get('patient_id'),
                'basename': data_item.get('basename'),
                'folder': data_item.get('folder'),
                'metadata': data_item.get('metadata', {}),
                'original_path': data_item.get('image'),
                'processed_shape': image_tensor.shape,
                'data_type': str(image_tensor.dtype)
            }
            
            results.append((item_id, storage_data))
            
        except Exception as e:
            print(f"é¢„å¤„ç†å¤±è´¥: {data_item.get('image', 'unknown')}, é”™è¯¯: {e}")
            continue
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•°æ®é¢„å¤„ç†å™¨')
    parser.add_argument('--config', required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰æ•°æ®')
    parser.add_argument('--output_dir', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--batch_size', type=int, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--num_workers', type=int, help='å¹¶è¡Œå¤„ç†è¿›ç¨‹æ•°')
    parser.add_argument('--enable_multiprocessing', action='store_true', help='å¯ç”¨å¤šè¿›ç¨‹å¤„ç†')
    parser.add_argument('--test_mode', type=int, help='æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç†æŒ‡å®šæ•°é‡çš„æ•°æ®é¡¹')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.output_dir:
        config['output_dir'] = args.output_dir
    if args.batch_size:
        config['preprocess_batch_size'] = args.batch_size
    if args.num_workers:
        config['num_workers'] = args.num_workers
    if args.enable_multiprocessing:
        config['enable_multiprocessing'] = True
    
    # é»˜è®¤é¢„å¤„ç†é…ç½®
    config.setdefault('preprocess_batch_size', 16)
    config.setdefault('num_workers', 1)  # é»˜è®¤å•è¿›ç¨‹
    config.setdefault('enable_multiprocessing', False)  # é»˜è®¤å…³é—­å¤šè¿›ç¨‹
    
    try:
        # åŠ è½½æ•°æ®åˆ—è¡¨
        print("ğŸ” åŠ è½½æ•°æ®...")
        df, cardiac_metric_columns = load_and_validate_csv_data(config)
        data_list = build_data_list(df, config, cardiac_metric_columns)
        
        if not data_list:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®")
            return
        
        # æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç†æŒ‡å®šæ•°é‡çš„æ•°æ®
        if args.test_mode:
            original_size = len(data_list)
            data_list = data_list[:args.test_mode]
            print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šä» {original_size} ä¸ªæ•°æ®é¡¹ä¸­é€‰æ‹©å‰ {len(data_list)} ä¸ªè¿›è¡Œå¤„ç†")
        
        # åˆ›å»ºé¢„å¤„ç†å™¨
        preprocessor = DataPreprocessor(config)
        
        # å¼€å§‹é¢„å¤„ç†
        print("ğŸš€ å¼€å§‹é¢„å¤„ç†æ•°æ®...")
        preprocessor.preprocess_data(data_list, force_reprocess=args.force)
        
        print("ğŸ‰ æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“ é¢„å¤„ç†æ•°æ®ä¿å­˜åœ¨: {preprocessor.hdf5_path}")
        print(f"ğŸ“‹ æ•°æ®ç´¢å¼•ä¿å­˜åœ¨: {preprocessor.metadata_path}")
        
    except Exception as e:
        print(f"âŒ é¢„å¤„ç†å¤±è´¥: {e}")
        raise


if __name__ == '__main__':
    main() 