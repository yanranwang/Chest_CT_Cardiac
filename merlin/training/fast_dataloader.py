#!/usr/bin/env python3
"""
Fast data loader - Efficiently read from HDF5 preprocessed data

This module provides efficient data loaders that directly read data from preprocessed HDF5 files,
significantly reducing training data loading time and improving GPU utilization.

Main features:
1. Direct reading from HDF5 preprocessed data
2. Support memory caching and preloading
3. Efficient multi-process data reading
4. Support data augmentation (optional)
5. Intelligent data splitting
6. Hybrid data loading (CSV labels + HDF5 images)
"""

import os
import h5py
import json
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import logging
from sklearn.model_selection import train_test_split
import threading
import queue
import warnings
warnings.filterwarnings('ignore')


class HybridCardiacDataset(Dataset):
    """æ··åˆå¿ƒè„åŠŸèƒ½æ•°æ®é›† - ä»CSVè¯»å–æ ‡ç­¾ï¼Œä»HDF5è¯»å–å›¾åƒæ•°æ®"""
    
    def __init__(self, 
                 csv_path: str,
                 hdf5_path: str, 
                 enable_cache: bool = True,
                 cache_size: int = 100,
                 label_columns: List[str] = None):
        """
        åˆå§‹åŒ–æ··åˆæ•°æ®é›†
        
        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«æ ‡ç­¾æ•°æ®
            hdf5_path: HDF5æ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«é¢„å¤„ç†çš„å›¾åƒæ•°æ®
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            cache_size: ç¼“å­˜å¤§å°
            label_columns: æ ‡ç­¾åˆ—ååˆ—è¡¨ï¼Œé»˜è®¤ä¸º['lvef', 'AS_maybe']
        """
        self.csv_path = csv_path
        self.hdf5_path = hdf5_path
        self.enable_cache = enable_cache
        self.cache_size = cache_size
        self.label_columns = label_columns or ['lvef', 'AS_maybe']
        
        # åˆå§‹åŒ–ç¼“å­˜
        if self.enable_cache:
            self._cache = {}
        
        # è¯»å–CSVæ•°æ®
        self.df = pd.read_csv(csv_path)
        print(f"ä»CSVæ–‡ä»¶è¯»å–äº† {len(self.df)} è¡Œæ•°æ®")
        
        # æ£€æŸ¥å¿…éœ€çš„åˆ—
        required_columns = ['basename', 'folder'] + self.label_columns
        missing_columns = [col for col in required_columns if col not in self.df.columns]
        if missing_columns:
            raise ValueError(f"CSVæ–‡ä»¶ä¸­ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")
        
        # æ¸…ç†æ•°æ®ï¼šç§»é™¤ç¼ºå¤±æ ‡ç­¾çš„è¡Œ
        initial_count = len(self.df)
        self.df = self.df.dropna(subset=self.label_columns)
        print(f"ç§»é™¤ç¼ºå¤±æ ‡ç­¾åå‰©ä½™ {len(self.df)} è¡Œæ•°æ®")
        
        if len(self.df) == 0:
            raise ValueError("æ¸…ç†åæ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®è¡Œ")
        
        # éªŒè¯HDF5æ–‡ä»¶
        if not os.path.exists(hdf5_path):
            raise FileNotFoundError(f"HDF5æ–‡ä»¶ä¸å­˜åœ¨: {hdf5_path}")
        
        # æ£€æŸ¥HDF5ä¸­çš„æ•°æ®é¡¹
        with h5py.File(hdf5_path, 'r') as f:
            if 'images' not in f:
                raise ValueError(f"HDF5æ–‡ä»¶ä¸­æ²¡æœ‰'images'ç»„: {hdf5_path}")
            self.hdf5_keys = set(f['images'].keys())
        
        print(f"HDF5æ–‡ä»¶ä¸­æœ‰ {len(self.hdf5_keys)} ä¸ªå›¾åƒæ•°æ®é¡¹")
        
        # å°è¯•åŠ è½½å…ƒæ•°æ®æ–‡ä»¶æ¥è·å–å“ˆå¸Œæ˜ å°„
        metadata_path = os.path.join(os.path.dirname(hdf5_path), 'data_metadata.json')
        hash_to_info = {}
        
        if os.path.exists(metadata_path):
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                if 'items' in metadata and isinstance(metadata['items'], list):
                    for item in metadata['items']:
                        if 'item_id' in item and 'basename' in item and 'folder' in item:
                            hash_to_info[item['item_id']] = {
                                'basename': item['basename'],
                                'folder': item['folder']
                            }
                    print(f"ä»å…ƒæ•°æ®æ–‡ä»¶åŠ è½½äº† {len(hash_to_info)} ä¸ªå“ˆå¸Œæ˜ å°„")
                else:
                    print("å…ƒæ•°æ®æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒï¼Œä½¿ç”¨ä¼ ç»Ÿæ ¼å¼")
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•è¯»å–å…ƒæ•°æ®æ–‡ä»¶ {metadata_path}: {e}")
        
        # æ„å»ºitem_idåˆ°CSVè¡Œçš„æ˜ å°„
        self.valid_items = []
        missing_in_hdf5 = []
        
        for idx, row in self.df.iterrows():
            basename = row['basename']
            folder = row['folder']
            
            # é¦–å…ˆå°è¯•ä¼ ç»Ÿæ ¼å¼ {folder}_{basename}
            traditional_item_id = f"{folder}_{basename}"
            
            # ç„¶åå°è¯•ä»å“ˆå¸Œæ˜ å°„ä¸­æŸ¥æ‰¾
            hash_item_id = None
            for hash_id, info in hash_to_info.items():
                if info['basename'] == basename and info['folder'] == folder:
                    hash_item_id = hash_id
                    break
            
            # ç¡®å®šå®é™…çš„item_id
            actual_item_id = None
            if traditional_item_id in self.hdf5_keys:
                actual_item_id = traditional_item_id
            elif hash_item_id and hash_item_id in self.hdf5_keys:
                actual_item_id = hash_item_id
            
            if actual_item_id:
                # æå–æ ‡ç­¾æ•°æ®
                labels = []
                for col in self.label_columns:
                    labels.append(float(row[col]))
                
                self.valid_items.append({
                    'item_id': actual_item_id,
                    'csv_idx': idx,
                    'basename': basename,
                    'folder': folder,
                    'labels': labels
                })
            else:
                missing_in_hdf5.append(f"{folder}_{basename}")
        
        print(f"åŒ¹é…åˆ° {len(self.valid_items)} ä¸ªæœ‰æ•ˆæ•°æ®é¡¹")
        
        if missing_in_hdf5:
            print(f"è­¦å‘Š: {len(missing_in_hdf5)} ä¸ªCSVæ¡ç›®åœ¨HDF5ä¸­æ‰¾ä¸åˆ°å¯¹åº”çš„å›¾åƒæ•°æ®")
            if len(missing_in_hdf5) <= 5:
                print("ç¼ºå¤±çš„item_id:", missing_in_hdf5)
        
        if len(self.valid_items) == 0:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°CSVå’ŒHDF5ä¸­éƒ½å­˜åœ¨çš„æœ‰æ•ˆæ•°æ®é¡¹")
        
        print(f"æœ€ç»ˆæ•°æ®é›†å¤§å°: {len(self.valid_items)} ä¸ªæ ·æœ¬")
        
        # æ‰“å°æ ‡ç­¾ç»Ÿè®¡ä¿¡æ¯
        self._print_label_stats()
    
    def _print_label_stats(self):
        """æ‰“å°æ ‡ç­¾ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“Š æ ‡ç­¾ç»Ÿè®¡ä¿¡æ¯:")
        for i, col in enumerate(self.label_columns):
            values = [item['labels'][i] for item in self.valid_items]
            print(f"{col}:")
            print(f"  å‡å€¼: {np.mean(values):.2f}")
            print(f"  æ ‡å‡†å·®: {np.std(values):.2f}")
            print(f"  èŒƒå›´: [{np.min(values):.2f}, {np.max(values):.2f}]")
            
            # å¦‚æœæ˜¯åˆ†ç±»æ ‡ç­¾ï¼ˆAS_maybeï¼‰ï¼Œæ˜¾ç¤ºåˆ†å¸ƒ
            if 'AS' in col.upper():
                unique_values = np.unique(values)
                if len(unique_values) <= 5:  # å‡è®¾æ˜¯åˆ†ç±»æ ‡ç­¾
                    for val in unique_values:
                        count = np.sum(np.array(values) == val)
                        percentage = (count / len(values)) * 100
                        print(f"  ç±»åˆ« {int(val)}: {count} æ ·æœ¬ ({percentage:.1f}%)")
    
    def __len__(self):
        return len(self.valid_items)
    
    def _get_from_cache(self, item_id: str):
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        if not self.enable_cache:
            return None
        return self._cache.get(item_id)
    
    def _add_to_cache(self, item_id: str, data: np.ndarray):
        """æ·»åŠ æ•°æ®åˆ°ç¼“å­˜"""
        if not self.enable_cache:
            return
            
        if len(self._cache) >= self.cache_size:
            # ç§»é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
            oldest_key = next(iter(self._cache))
            del self._cache[oldest_key]
        self._cache[item_id] = data
    
    def _load_image_from_hdf5(self, item_id: str) -> np.ndarray:
        """ä»HDF5æ–‡ä»¶åŠ è½½å›¾åƒæ•°æ®"""
        # æ£€æŸ¥ç¼“å­˜
        cached_data = self._get_from_cache(item_id)
        if cached_data is not None:
            return cached_data
        
        # ä»HDF5æ–‡ä»¶åŠ è½½
        with h5py.File(self.hdf5_path, 'r') as f:
            image_data = f['images'][item_id][:]
        
        # æ·»åŠ åˆ°ç¼“å­˜
        self._add_to_cache(item_id, image_data)
        
        return image_data
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        item_info = self.valid_items[idx]
        item_id = item_info['item_id']
        
        # ä»HDF5åŠ è½½å›¾åƒæ•°æ®
        image_data = self._load_image_from_hdf5(item_id)
        
        # è½¬æ¢ä¸ºtensor
        image_tensor = torch.from_numpy(image_data).float()
        
        # æ„å»ºæ ‡ç­¾tensor
        labels_tensor = torch.tensor(item_info['labels'], dtype=torch.float32)
        
        return {
            'image': image_tensor,
            'cardiac_metrics': labels_tensor,  # ä¿æŒä¸åŸæœ‰æ¥å£ä¸€è‡´
            'labels': labels_tensor,  # é¢å¤–æä¾›labelså­—æ®µ
            'patient_id': item_info['basename'],
            'basename': item_info['basename'],
            'folder': item_info['folder'],
            'item_id': item_id
        }


class FastCardiacDataset(Dataset):
    """Fast cardiac function dataset - Read preprocessed data from HDF5 files"""
    
    def __init__(self, 
                 hdf5_path: str, 
                 metadata_path: str,
                 item_ids: List[str],
                 enable_cache: bool = True,
                 cache_size: int = 1000,
                 preload_data: bool = False):
        """
        Initialize fast dataset
        
        Args:
            hdf5_path: HDF5 file path
            metadata_path: Metadata file path
            item_ids: List of data item IDs
            enable_cache: Whether to enable memory cache
            cache_size: Cache size
            preload_data: Whether to preload all data to memory
        """
        self.hdf5_path = Path(hdf5_path)
        self.metadata_path = Path(metadata_path)
        self.item_ids = item_ids
        self.enable_cache = enable_cache
        self.cache_size = cache_size
        self.preload_data = preload_data
        
        # Validate file existence
        if not self.hdf5_path.exists():
            raise FileNotFoundError(f"HDF5 file not found: {hdf5_path}")
        if not self.metadata_path.exists():
            raise FileNotFoundError(f"Metadata file not found: {metadata_path}")
        
        # Load metadata index
        self.metadata_index = self._load_metadata_index()
        
        # Validate item IDs
        self._validate_item_ids()
        
        # Setup logging first (before preloading data)
        self.logger = logging.getLogger(__name__)
        
        # Initialize cache
        self._cache = {} if enable_cache else None
        self._cache_lock = threading.Lock() if enable_cache else None
        
        # Preload data
        if preload_data:
            self._preload_all_data()
    
    def _load_metadata_index(self) -> Dict[str, Any]:
        """Load metadata index"""
        try:
            with open(self.metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # Validate metadata structure
            if not isinstance(metadata, dict):
                raise ValueError(f"Metadata file should contain a dict, got {type(metadata)}")
            
            required_keys = ['items']
            missing_keys = [key for key in required_keys if key not in metadata]
            if missing_keys:
                raise ValueError(f"Metadata file missing required keys: {missing_keys}")
            
            return metadata
            
        except json.JSONDecodeError as e:
            raise ValueError(f"Failed to parse metadata file {self.metadata_path}: {e}")
        except Exception as e:
            raise ValueError(f"Failed to load metadata file {self.metadata_path}: {e}")
    
    def _validate_item_ids(self):
        """Validate item IDs"""
        items = self.metadata_index.get('items')
        if items is None:
            raise ValueError("Metadata index does not contain 'items' key")
        
        if isinstance(items, list):
            # If items is a list, this indicates an incorrect metadata format
            raise ValueError(
                "å…ƒæ•°æ®æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ã€‚'items' åº”è¯¥æ˜¯å­—å…¸è€Œä¸æ˜¯åˆ—è¡¨ã€‚\n"
                "è¿™é€šå¸¸è¡¨ç¤ºæ•°æ®é¢„å¤„ç†æ²¡æœ‰æ­£ç¡®å®Œæˆã€‚è¯·é‡æ–°è¿è¡Œæ•°æ®é¢„å¤„ç†ï¼š\n"
                "python -m merlin.training.data_preprocessor --config config.json --force"
            )
        elif isinstance(items, dict):
            # If items is a dict, use the keys
            available_ids = set(items.keys())
        else:
            raise ValueError(f"Unexpected type for metadata 'items': {type(items)}. Expected dict or list.")
        
        missing_ids = set(self.item_ids) - available_ids
        
        if missing_ids:
            raise ValueError(f"Following item IDs not found: {missing_ids}")
    
    def _preload_all_data(self):
        """Preload all data to memory"""
        self.logger.info(f"Preloading {len(self.item_ids)} data items...")
        
        self._preloaded_data = {}
        
        with h5py.File(self.hdf5_path, 'r') as f:
            images_group = f['images']
            metadata_group = f['metadata']
            
            for item_id in self.item_ids:
                # Load image data
                image_data = images_group[item_id][:]
                
                # Load metadata
                metadata_str = metadata_group[item_id][()]
                if isinstance(metadata_str, bytes):
                    metadata_str = metadata_str.decode('utf-8')
                metadata = json.loads(metadata_str)
                
                self._preloaded_data[item_id] = {
                    'image': image_data,
                    'metadata': metadata
                }
        
        self.logger.info("Data preloading completed")
    
    def _get_from_cache(self, item_id: str) -> Optional[Dict[str, Any]]:
        """Get data from cache"""
        if not self.enable_cache:
            return None
        
        with self._cache_lock:
            return self._cache.get(item_id)
    
    def _add_to_cache(self, item_id: str, data: Dict[str, Any]):
        """Add data to cache"""
        if not self.enable_cache:
            return
        
        with self._cache_lock:
            if len(self._cache) >= self.cache_size:
                # Remove oldest cache item
                oldest_key = next(iter(self._cache))
                del self._cache[oldest_key]
            
            self._cache[item_id] = data
    
    def _load_item_from_hdf5(self, item_id: str) -> Dict[str, Any]:
        """Load data item from HDF5 file"""
        if self.preload_data:
            return self._preloaded_data[item_id]
        
        # Check cache
        cached_data = self._get_from_cache(item_id)
        if cached_data is not None:
            return cached_data
        
        # Load from HDF5 file
        with h5py.File(self.hdf5_path, 'r') as f:
            images_group = f['images']
            metadata_group = f['metadata']
            
            # Load image data
            image_data = images_group[item_id][:]
            
            # Load metadata
            metadata_str = metadata_group[item_id][()]
            if isinstance(metadata_str, bytes):
                metadata_str = metadata_str.decode('utf-8')
            metadata = json.loads(metadata_str)
            
            data = {
                'image': image_data,
                'metadata': metadata
            }
            
            # Add to cache
            self._add_to_cache(item_id, data)
            
            return data
    
    def __len__(self) -> int:
        return len(self.item_ids)
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        item_id = self.item_ids[idx]
        data = self._load_item_from_hdf5(item_id)
        
        # Convert to tensor
        image_tensor = torch.from_numpy(data['image']).float()
        
        # Get cardiac function metrics
        cardiac_metrics = data['metadata']['cardiac_metrics']
        if cardiac_metrics is not None:
            cardiac_metrics = torch.tensor(cardiac_metrics, dtype=torch.float32)
        else:
            # å¦‚æœæ²¡æœ‰çœŸå®æ ‡ç­¾ï¼ŒæŠ›å‡ºé”™è¯¯è€Œä¸æ˜¯ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            raise ValueError(f"æ ·æœ¬ {item_id} ç¼ºå°‘å¿ƒè„åŠŸèƒ½æ ‡ç­¾æ•°æ®ã€‚è¯·ç¡®ä¿é¢„å¤„ç†æ•°æ®ä¸­åŒ…å«æœ‰æ•ˆçš„cardiac_metricsã€‚")
        
        return {
            'image': image_tensor,
            'cardiac_metrics': cardiac_metrics,
            'patient_id': data['metadata']['patient_id'],
            'basename': data['metadata']['basename'],
            'folder': data['metadata']['folder']
        }
    



class FastDataLoaderManager:
    """Fast data loader manager"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # File paths
        self.hdf5_path = Path(config['preprocessed_data_dir']) / 'preprocessed_data.h5'
        self.metadata_path = Path(config['preprocessed_data_dir']) / 'data_metadata.json'
        
        # Validate file existence
        if not self.hdf5_path.exists():
            raise FileNotFoundError(f"Preprocessed data file not found: {self.hdf5_path}")
        if not self.metadata_path.exists():
            raise FileNotFoundError(f"Metadata file not found: {self.metadata_path}")
        
        # Load metadata
        self.metadata_index = self._load_metadata_index()
        
        # Debug metadata structure
        self.logger.info(f"Metadata index keys: {list(self.metadata_index.keys())}")
        items = self.metadata_index.get('items')
        if items is not None:
            self.logger.info(f"Items type: {type(items)}, length: {len(items)}")
            if isinstance(items, dict):
                self.logger.info(f"Loaded metadata index: {len(items)} data items")
            elif isinstance(items, list):
                self.logger.info(f"Loaded metadata index: {len(items)} data items (list format)")
            else:
                self.logger.error(f"Unexpected items type: {type(items)}")
        else:
            self.logger.error("No 'items' key found in metadata index")
    
    def _load_metadata_index(self) -> Dict[str, Any]:
        """Load metadata index"""
        try:
            with open(self.metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # Validate metadata structure
            if not isinstance(metadata, dict):
                raise ValueError(f"Metadata file should contain a dict, got {type(metadata)}")
            
            required_keys = ['items']
            missing_keys = [key for key in required_keys if key not in metadata]
            if missing_keys:
                raise ValueError(f"Metadata file missing required keys: {missing_keys}")
            
            return metadata
            
        except json.JSONDecodeError as e:
            raise ValueError(f"Failed to parse metadata file {self.metadata_path}: {e}")
        except Exception as e:
            raise ValueError(f"Failed to load metadata file {self.metadata_path}: {e}")
    
    def get_all_item_ids(self) -> List[str]:
        """Get all data item IDs"""
        items = self.metadata_index.get('items')
        if items is None:
            raise ValueError("Metadata index does not contain 'items' key")
        
        if isinstance(items, list):
            # If items is a list, this indicates an incorrect metadata format
            self.logger.error("Metadata 'items' is a list, but should be a dict. This indicates corrupted or incomplete preprocessing.")
            raise ValueError(
                "å…ƒæ•°æ®æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ã€‚'items' åº”è¯¥æ˜¯å­—å…¸è€Œä¸æ˜¯åˆ—è¡¨ã€‚\n"
                "è¿™é€šå¸¸è¡¨ç¤ºæ•°æ®é¢„å¤„ç†æ²¡æœ‰æ­£ç¡®å®Œæˆã€‚è¯·é‡æ–°è¿è¡Œæ•°æ®é¢„å¤„ç†ï¼š\n"
                "python -m merlin.training.data_preprocessor --config config.json --force"
            )
        elif isinstance(items, dict):
            # If items is a dict, return the keys
            return list(items.keys())
        else:
            raise ValueError(f"Unexpected type for metadata 'items': {type(items)}. Expected dict or list.")
    
    def get_items_by_patient(self, patient_id: str) -> List[str]:
        """Get data items by patient ID"""
        patient_mapping = self.metadata_index.get('patient_mapping', {})
        if not isinstance(patient_mapping, dict):
            self.logger.warning("Patient mapping is not available or not a dict")
            return []
        return patient_mapping.get(patient_id, [])
    
    def get_items_by_folder(self, folder: str) -> List[str]:
        """Get data items by folder"""
        folder_mapping = self.metadata_index.get('folder_mapping', {})
        if not isinstance(folder_mapping, dict):
            self.logger.warning("Folder mapping is not available or not a dict")
            return []
        return folder_mapping.get(folder, [])
    
    def split_data(self, 
                   split_method: str = 'random',
                   train_ratio: float = 0.8,
                   random_state: int = 42) -> Tuple[List[str], List[str]]:
        """Split data into training and validation sets"""
        all_item_ids = self.get_all_item_ids()
        
        if split_method == 'random':
            # Random split
            train_ids, val_ids = train_test_split(
                all_item_ids,
                train_size=train_ratio,
                random_state=random_state,
                shuffle=True
            )
        
        elif split_method == 'patient_based':
            # Patient-based split
            all_patients = list(self.metadata_index['patient_mapping'].keys())
            train_patients, val_patients = train_test_split(
                all_patients,
                train_size=train_ratio,
                random_state=random_state,
                shuffle=True
            )
            
            train_ids = []
            val_ids = []
            
            for patient in train_patients:
                train_ids.extend(self.get_items_by_patient(patient))
            
            for patient in val_patients:
                val_ids.extend(self.get_items_by_patient(patient))
        
        elif split_method == 'sequential':
            # Sequential split
            split_idx = int(len(all_item_ids) * train_ratio)
            train_ids = all_item_ids[:split_idx]
            val_ids = all_item_ids[split_idx:]
        
        else:
            raise ValueError(f"Unsupported split method: {split_method}")
        
        self.logger.info(f"Data split completed: Training set {len(train_ids)}, Validation set {len(val_ids)}")
        return train_ids, val_ids
    
    def create_dataset(self, 
                      item_ids: List[str],
                      enable_cache: bool = True,
                      cache_size: int = 1000,
                      preload_data: bool = False) -> FastCardiacDataset:
        """Create dataset"""
        return FastCardiacDataset(
            hdf5_path=str(self.hdf5_path),
            metadata_path=str(self.metadata_path),
            item_ids=item_ids,
            enable_cache=enable_cache,
            cache_size=cache_size,
            preload_data=preload_data
        )
    
    def create_dataloaders(self,
                          split_method: str = 'random',
                          train_ratio: float = 0.8,
                          batch_size: int = 4,
                          num_workers: int = 4,
                          enable_cache: bool = True,
                          cache_size: int = 1000,
                          preload_data: bool = False,
                          random_state: int = 42) -> Tuple[DataLoader, DataLoader]:
        """Create training and validation data loaders"""
        
        # Split data
        train_ids, val_ids = self.split_data(
            split_method=split_method,
            train_ratio=train_ratio,
            random_state=random_state
        )
        
        self.logger.info(f"Data split completed: {len(train_ids)} training, {len(val_ids)} validation")
        
        # Create datasets
        train_dataset = FastCardiacDataset(
            hdf5_path=str(self.hdf5_path),
            metadata_path=str(self.metadata_path),
            item_ids=train_ids,
            enable_cache=enable_cache,
            cache_size=cache_size,
            preload_data=preload_data
        )
        
        val_dataset = FastCardiacDataset(
            hdf5_path=str(self.hdf5_path),
            metadata_path=str(self.metadata_path),
            item_ids=val_ids,
            enable_cache=enable_cache,
            cache_size=cache_size,
            preload_data=preload_data
        ) if val_ids else None
        
        # Create data loaders
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True,
            drop_last=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
            drop_last=False
        ) if val_dataset else None
        
        return train_loader, val_loader
    
    def create_hybrid_dataloaders(self,
                                 csv_path: str,
                                 label_columns: List[str] = None,
                                 split_method: str = 'random',
                                 train_ratio: float = 0.8,
                                 batch_size: int = 4,
                                 num_workers: int = 4,
                                 enable_cache: bool = True,
                                 cache_size: int = 100,
                                 random_state: int = 42) -> Tuple[DataLoader, DataLoader]:
        """
        åˆ›å»ºæ··åˆæ•°æ®åŠ è½½å™¨ - ä»CSVè¯»å–æ ‡ç­¾ï¼Œä»HDF5è¯»å–å›¾åƒ
        
        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«æ ‡ç­¾æ•°æ®
            label_columns: æ ‡ç­¾åˆ—ååˆ—è¡¨ï¼Œé»˜è®¤ä¸º['lvef', 'AS_maybe']
            split_method: æ•°æ®åˆ†å‰²æ–¹æ³• ('random' æˆ– 'patient_based')
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            batch_size: æ‰¹é‡å¤§å°
            num_workers: æ•°æ®åŠ è½½è¿›ç¨‹æ•°
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            cache_size: ç¼“å­˜å¤§å°
            random_state: éšæœºç§å­
            
        Returns:
            train_loader, val_loader: è®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
        """
        
        print("=" * 80)
        print("ğŸ”„ åˆ›å»ºæ··åˆæ•°æ®åŠ è½½å™¨ (CSVæ ‡ç­¾ + HDF5å›¾åƒ)")
        print("=" * 80)
        print(f"ğŸ“Š CSVæ ‡ç­¾æ–‡ä»¶: {csv_path}")
        print(f"ğŸ–¼ï¸ HDF5å›¾åƒæ–‡ä»¶: {self.hdf5_path}")
        
        # éªŒè¯CSVæ–‡ä»¶å­˜åœ¨
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        
        # åˆ›å»ºå®Œæ•´æ•°æ®é›†
        full_dataset = HybridCardiacDataset(
            csv_path=csv_path,
            hdf5_path=str(self.hdf5_path),
            enable_cache=enable_cache,
            cache_size=cache_size,
            label_columns=label_columns
        )
        
        print(f"âœ… æˆåŠŸåˆ›å»ºæ•°æ®é›†ï¼Œå…± {len(full_dataset)} ä¸ªæ ·æœ¬")
        
        # æ•°æ®åˆ†å‰²
        all_items = full_dataset.valid_items
        
        if split_method == 'random':
            # éšæœºåˆ†å‰²
            train_items, val_items = train_test_split(
                all_items,
                train_size=train_ratio,
                random_state=random_state,
                shuffle=True
            )
        elif split_method == 'patient_based':
            # åŸºäºæ‚£è€…çš„åˆ†å‰²
            patient_items = {}
            for item in all_items:
                patient_id = item['basename']  # ä½¿ç”¨basenameä½œä¸ºpatient_id
                if patient_id not in patient_items:
                    patient_items[patient_id] = []
                patient_items[patient_id].append(item)
            
            patients = list(patient_items.keys())
            train_patients, val_patients = train_test_split(
                patients,
                train_size=train_ratio,
                random_state=random_state,
                shuffle=True
            )
            
            train_items = []
            val_items = []
            for patient in train_patients:
                train_items.extend(patient_items[patient])
            for patient in val_patients:
                val_items.extend(patient_items[patient])
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®åˆ†å‰²æ–¹æ³•: {split_method}")
        
        print(f"ğŸ“Š æ•°æ®åˆ†å‰²å®Œæˆ:")
        print(f"   è®­ç»ƒé›†: {len(train_items)} ä¸ªæ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(val_items)} ä¸ªæ ·æœ¬")
        print(f"   åˆ†å‰²æ–¹æ³•: {split_method}")
        
        # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
        train_dataset = HybridCardiacDataset.__new__(HybridCardiacDataset)
        train_dataset.__dict__.update(full_dataset.__dict__)
        train_dataset.valid_items = train_items
        
        val_dataset = None
        if val_items:
            val_dataset = HybridCardiacDataset.__new__(HybridCardiacDataset)
            val_dataset.__dict__.update(full_dataset.__dict__)
            val_dataset.valid_items = val_items
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True,
            drop_last=True
        )
        
        val_loader = None
        if val_dataset:
            val_loader = DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=num_workers,
                pin_memory=True,
                drop_last=False
            )
        
        # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
        self._print_hybrid_label_stats(train_items, val_items, full_dataset.label_columns)
        
        print("=" * 80)
        
        return train_loader, val_loader
    
    def _print_hybrid_label_stats(self, train_items: List[Dict], val_items: List[Dict], label_columns: List[str]):
        """æ‰“å°æ··åˆæ•°æ®é›†çš„æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡"""
        print(f"\nğŸ“ˆ æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡:")
        
        # è®­ç»ƒé›†ç»Ÿè®¡
        if train_items:
            print("è®­ç»ƒé›†:")
            for i, col in enumerate(label_columns):
                values = [item['labels'][i] for item in train_items]
                print(f"  {col}:")
                print(f"    å‡å€¼: {np.mean(values):.2f}")
                print(f"    æ ‡å‡†å·®: {np.std(values):.2f}")
                print(f"    èŒƒå›´: [{np.min(values):.2f}, {np.max(values):.2f}]")
                
                # å¦‚æœæ˜¯åˆ†ç±»æ ‡ç­¾ï¼Œæ˜¾ç¤ºåˆ†å¸ƒ
                if 'AS' in col.upper():
                    unique_values = np.unique(values)
                    if len(unique_values) <= 5:
                        for val in unique_values:
                            count = np.sum(np.array(values) == val)
                            percentage = (count / len(values)) * 100
                            print(f"    ç±»åˆ« {int(val)}: {count} æ ·æœ¬ ({percentage:.1f}%)")
        
        # éªŒè¯é›†ç»Ÿè®¡
        if val_items:
            print("éªŒè¯é›†:")
            for i, col in enumerate(label_columns):
                values = [item['labels'][i] for item in val_items]
                print(f"  {col}:")
                print(f"    å‡å€¼: {np.mean(values):.2f}")
                print(f"    æ ‡å‡†å·®: {np.std(values):.2f}")
                print(f"    èŒƒå›´: [{np.min(values):.2f}, {np.max(values):.2f}]")
                
                # å¦‚æœæ˜¯åˆ†ç±»æ ‡ç­¾ï¼Œæ˜¾ç¤ºåˆ†å¸ƒ
                if 'AS' in col.upper():
                    unique_values = np.unique(values)
                    if len(unique_values) <= 5:
                        for val in unique_values:
                            count = np.sum(np.array(values) == val)
                            percentage = (count / len(values)) * 100
                            print(f"    ç±»åˆ« {int(val)}: {count} æ ·æœ¬ ({percentage:.1f}%)")
    
    def get_data_statistics(self) -> Dict[str, Any]:
        """Get data statistics"""
        stats = self.metadata_index['statistics'].copy()
        
        # Add file size information
        hdf5_size = self.hdf5_path.stat().st_size / (1024 * 1024)  # MB
        metadata_size = self.metadata_path.stat().st_size / 1024  # KB
        
        stats.update({
            'hdf5_file_size_mb': round(hdf5_size, 2),
            'metadata_file_size_kb': round(metadata_size, 2),
            'data_loading_method': 'HDF5_Fast_Loading'
        })
        
        return stats


def check_preprocessing_status(config: Dict[str, Any]) -> Dict[str, Any]:
    """Check the status of preprocessing data"""
    preprocessed_dir = config.get('preprocessed_data_dir', '')
    if not preprocessed_dir:
        return {
            'status': 'missing_config',
            'message': 'preprocessed_data_dir not specified in config'
        }
    
    hdf5_path = Path(preprocessed_dir) / 'preprocessed_data.h5'
    metadata_path = Path(preprocessed_dir) / 'data_metadata.json'
    
    status = {
        'preprocessed_dir': preprocessed_dir,
        'hdf5_exists': hdf5_path.exists(),
        'metadata_exists': metadata_path.exists(),
        'hdf5_path': str(hdf5_path),
        'metadata_path': str(metadata_path)
    }
    
    if not status['hdf5_exists'] or not status['metadata_exists']:
        status['status'] = 'files_missing'
        return status
    
    # Check metadata format
    try:
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        if not isinstance(metadata, dict):
            status['status'] = 'invalid_metadata_format'
            status['metadata_type'] = str(type(metadata))
            return status
        
        if 'items' not in metadata:
            status['status'] = 'missing_items_key'
            status['metadata_keys'] = list(metadata.keys())
            return status
        
        items = metadata['items']
        if not isinstance(items, dict):
            status['status'] = 'invalid_items_format'
            status['items_type'] = str(type(items))
            return status
        
        status['status'] = 'ok'
        status['num_items'] = len(items)
        return status
        
    except Exception as e:
        status['status'] = 'metadata_error'
        status['error'] = str(e)
        return status


def create_fast_data_loaders(config: Dict[str, Any]) -> Tuple[DataLoader, DataLoader]:
    """Create convenient function for fast data loaders"""
    try:
        manager = FastDataLoaderManager(config)
        return manager.create_dataloaders(
            split_method=config.get('split_method', 'random'),
            train_ratio=config.get('train_val_split', 0.8),
            batch_size=config.get('batch_size', 4),
            num_workers=config.get('num_workers', 4),
            enable_cache=config.get('cache_config', {}).get('enable_cache', True),
            cache_size=config.get('cache_config', {}).get('cache_size', 1000),
            preload_data=config.get('cache_config', {}).get('preload_train_data', False),
            random_state=config.get('seed', 42)
        )
    except Exception as e:
        print("\n" + "=" * 80)
        print("âŒ å¿«é€Ÿæ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å¤±è´¥")
        print("=" * 80)
        print(f"é”™è¯¯: {e}")
        
        # Check preprocessing status
        status = check_preprocessing_status(config)
        print(f"\nğŸ” é¢„å¤„ç†æ•°æ®çŠ¶æ€æ£€æŸ¥:")
        print(f"   ç›®å½•: {status.get('preprocessed_dir', 'N/A')}")
        print(f"   HDF5æ–‡ä»¶å­˜åœ¨: {status.get('hdf5_exists', False)}")
        print(f"   å…ƒæ•°æ®æ–‡ä»¶å­˜åœ¨: {status.get('metadata_exists', False)}")
        
        if status['status'] == 'files_missing':
            print("\nâŒ é¢„å¤„ç†æ•°æ®æ–‡ä»¶ç¼ºå¤±")
        elif status['status'] == 'invalid_metadata_format':
            print(f"\nâŒ å…ƒæ•°æ®æ–‡ä»¶æ ¼å¼é”™è¯¯ (ç±»å‹: {status.get('metadata_type', 'unknown')})")
        elif status['status'] == 'missing_items_key':
            print(f"\nâŒ å…ƒæ•°æ®æ–‡ä»¶ç¼ºå°‘'items'é”® (ç°æœ‰é”®: {status.get('metadata_keys', [])})")
        elif status['status'] == 'invalid_items_format':
            print(f"\nâŒ å…ƒæ•°æ®ä¸­'items'æ ¼å¼é”™è¯¯ (ç±»å‹: {status.get('items_type', 'unknown')}ï¼Œåº”ä¸ºdict)")
        elif status['status'] == 'ok':
            print(f"\nâœ… é¢„å¤„ç†æ•°æ®æ ¼å¼æ­£ç¡® ({status.get('num_items', 0)} ä¸ªæ•°æ®é¡¹)")
        
        print("\nğŸ” è§£å†³æ–¹æ¡ˆ:")
        print("1. è¿è¡Œæˆ–é‡æ–°è¿è¡Œæ•°æ®é¢„å¤„ç†:")
        print("   python -m merlin.training.data_preprocessor --config config.json --force")
        print("2. ç¡®ä¿CSVæ–‡ä»¶è·¯å¾„æ­£ç¡®")
        print("3. ç¡®ä¿å›¾åƒæ–‡ä»¶è·¯å¾„æ­£ç¡®")
        print("4. æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦è¶³å¤Ÿ")
        print("=" * 80)
        raise


def benchmark_data_loading(config: Dict[str, Any], num_batches: int = 10) -> Dict[str, float]:
    """Benchmark data loading performance"""
    import time
    
    manager = FastDataLoaderManager(config)
    train_loader, val_loader = manager.create_dataloaders(
        split_method=config.get('split_method', 'random'),
        train_ratio=config.get('train_val_split', 0.8),
        batch_size=config.get('batch_size', 4),
        num_workers=config.get('num_workers', 4),
        enable_cache=config.get('cache_config', {}).get('enable_cache', True),
        cache_size=config.get('cache_config', {}).get('cache_size', 1000),
        preload_data=config.get('cache_config', {}).get('preload_train_data', False),
        random_state=config.get('seed', 42)
    )
    
    # Test training data loading
    start_time = time.time()
    for i, batch in enumerate(train_loader):
        if i >= num_batches:
            break
        # Simulated data usage
        _ = batch['image'].shape
    
    train_time = time.time() - start_time
    
    # Test validation data loading
    start_time = time.time()
    for i, batch in enumerate(val_loader):
        if i >= num_batches:
            break
        # Simulated data usage
        _ = batch['image'].shape
    
    val_time = time.time() - start_time
    
    return {
        'train_loading_time': train_time,
        'val_loading_time': val_time,
        'train_batches_per_second': num_batches / train_time,
        'val_batches_per_second': num_batches / val_time
    }


if __name__ == '__main__':
    # Test fast data loader
    config = {
        'preprocessed_data_dir': '/data/joycewyr/cardiac_training_fast',
        'batch_size': 4,
        'num_workers': 4,
        'split_method': 'random',
        'train_val_split': 0.8,
        'seed': 42,
        'cache_config': {
            'enable_cache': True,
            'cache_size': 1000,
            'preload_train_data': False,
            'preload_val_data': False
        }
    }
    
    try:
        # Create data loaders
        train_loader, val_loader = create_fast_data_loaders(config)
        
        print(f"Training set size: {len(train_loader.dataset)}")
        print(f"Validation set size: {len(val_loader.dataset)}")
        
        # Test data loading
        batch = next(iter(train_loader))
        print(f"Batch shape: {batch['image'].shape}")
        print(f"Cardiac function metrics shape: {batch['cardiac_metrics'].shape}")
        
        # Performance benchmark test
        print("\nStarting performance benchmark test...")
        benchmark_results = benchmark_data_loading(config)
        
        print("Benchmark test results:")
        for key, value in benchmark_results.items():
            print(f"  {key}: {value:.4f}")
        
    except Exception as e:
        print(f"Test failed: {e}")
        print("Please run data preprocessing script to generate HDF5 file first")


def create_hybrid_dataloaders_from_config(config: Dict[str, Any]) -> Tuple[DataLoader, DataLoader]:
    """
    ä»é…ç½®åˆ›å»ºæ··åˆæ•°æ®åŠ è½½å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        config: é…ç½®å­—å…¸ï¼Œå¿…é¡»åŒ…å«ä»¥ä¸‹é”®ï¼š
            - csv_path: CSVæ ‡ç­¾æ–‡ä»¶è·¯å¾„
            - hdf5_path: HDF5å›¾åƒæ–‡ä»¶è·¯å¾„ (æˆ– preprocessed_data_dir)
            - label_columns: æ ‡ç­¾åˆ—ååˆ—è¡¨ (å¯é€‰ï¼Œé»˜è®¤['lvef', 'AS_maybe'])
            - split_method: æ•°æ®åˆ†å‰²æ–¹æ³• (å¯é€‰ï¼Œé»˜è®¤'random')
            - train_val_split: è®­ç»ƒé›†æ¯”ä¾‹ (å¯é€‰ï¼Œé»˜è®¤0.8)
            - batch_size: æ‰¹é‡å¤§å° (å¯é€‰ï¼Œé»˜è®¤4)
            - num_workers: æ•°æ®åŠ è½½è¿›ç¨‹æ•° (å¯é€‰ï¼Œé»˜è®¤4)
            - cache_size: ç¼“å­˜å¤§å° (å¯é€‰ï¼Œé»˜è®¤100)
            - seed: éšæœºç§å­ (å¯é€‰ï¼Œé»˜è®¤42)
    
    Returns:
        train_loader, val_loader: è®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
    """
    
    # æ£€æŸ¥å¿…éœ€çš„é…ç½®
    if 'csv_path' not in config:
        raise ValueError("é…ç½®ä¸­ç¼ºå°‘csv_path")
    
    # ç¡®å®šHDF5æ–‡ä»¶è·¯å¾„
    if 'hdf5_path' in config:
        hdf5_path = config['hdf5_path']
    elif 'preprocessed_data_dir' in config:
        hdf5_path = str(Path(config['preprocessed_data_dir']) / 'preprocessed_data.h5')
    else:
        raise ValueError("é…ç½®ä¸­å¿…é¡»åŒ…å«hdf5_pathæˆ–preprocessed_data_dir")
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(config['csv_path']):
        raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {config['csv_path']}")
    if not os.path.exists(hdf5_path):
        raise FileNotFoundError(f"HDF5æ–‡ä»¶ä¸å­˜åœ¨: {hdf5_path}")
    
    print("=" * 80)
    print("ğŸš€ åˆ›å»ºæ··åˆæ•°æ®åŠ è½½å™¨ (ä»é…ç½®)")
    print("=" * 80)
    print(f"ğŸ“Š CSVæ ‡ç­¾æ–‡ä»¶: {config['csv_path']}")
    print(f"ğŸ–¼ï¸ HDF5å›¾åƒæ–‡ä»¶: {hdf5_path}")
    
    # ç›´æ¥åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨ï¼Œä¸éœ€è¦FastDataLoaderManager
    # åˆ›å»ºå®Œæ•´æ•°æ®é›†
    full_dataset = HybridCardiacDataset(
        csv_path=config['csv_path'],
        hdf5_path=hdf5_path,
        enable_cache=config.get('enable_cache', True),
        cache_size=config.get('cache_size', 100),
        label_columns=config.get('label_columns', ['lvef', 'AS_maybe'])
    )
    
    print(f"âœ… æˆåŠŸåˆ›å»ºæ•°æ®é›†ï¼Œå…± {len(full_dataset)} ä¸ªæ ·æœ¬")
    
    # æ•°æ®åˆ†å‰²
    split_method = config.get('split_method', 'random')
    train_ratio = config.get('train_val_split', 0.8)
    random_state = config.get('seed', 42)
    
    all_items = full_dataset.valid_items
    
    if split_method == 'random':
        train_items, val_items = train_test_split(
            all_items,
            train_size=train_ratio,
            random_state=random_state,
            shuffle=True
        )
    elif split_method == 'patient_based':
        # åŸºäºæ‚£è€…çš„åˆ†å‰²
        patient_items = {}
        for item in all_items:
            patient_id = item['basename']
            if patient_id not in patient_items:
                patient_items[patient_id] = []
            patient_items[patient_id].append(item)
        
        patients = list(patient_items.keys())
        train_patients, val_patients = train_test_split(
            patients,
            train_size=train_ratio,
            random_state=random_state,
            shuffle=True
        )
        
        train_items = []
        val_items = []
        for patient in train_patients:
            train_items.extend(patient_items[patient])
        for patient in val_patients:
            val_items.extend(patient_items[patient])
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®åˆ†å‰²æ–¹æ³•: {split_method}")
    
    print(f"ğŸ“Š æ•°æ®åˆ†å‰²å®Œæˆ:")
    print(f"   è®­ç»ƒé›†: {len(train_items)} ä¸ªæ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(val_items)} ä¸ªæ ·æœ¬")
    print(f"   åˆ†å‰²æ–¹æ³•: {split_method}")
    
    # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
    train_dataset = HybridCardiacDataset.__new__(HybridCardiacDataset)
    train_dataset.__dict__.update(full_dataset.__dict__)
    train_dataset.valid_items = train_items
    
    val_dataset = None
    if val_items:
        val_dataset = HybridCardiacDataset.__new__(HybridCardiacDataset)
        val_dataset.__dict__.update(full_dataset.__dict__)
        val_dataset.valid_items = val_items
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = config.get('batch_size', 4)
    num_workers = config.get('num_workers', 4)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True
    )
    
    val_loader = None
    if val_dataset:
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
            drop_last=False
        )
    
    # æ‰“å°æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡
    _print_label_distribution_stats(train_items, val_items, full_dataset.label_columns)
    
    print("=" * 80)
    
    return train_loader, val_loader


def _print_label_distribution_stats(train_items: List[Dict], val_items: List[Dict], label_columns: List[str]):
    """æ‰“å°æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡çš„è¾…åŠ©å‡½æ•°"""
    print(f"\nğŸ“ˆ æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡:")
    
    # è®­ç»ƒé›†ç»Ÿè®¡
    if train_items:
        print("è®­ç»ƒé›†:")
        for i, col in enumerate(label_columns):
            values = [item['labels'][i] for item in train_items]
            print(f"  {col}:")
            print(f"    å‡å€¼: {np.mean(values):.2f}")
            print(f"    æ ‡å‡†å·®: {np.std(values):.2f}")
            print(f"    èŒƒå›´: [{np.min(values):.2f}, {np.max(values):.2f}]")
            
            # å¦‚æœæ˜¯åˆ†ç±»æ ‡ç­¾ï¼Œæ˜¾ç¤ºåˆ†å¸ƒ
            if 'AS' in col.upper():
                unique_values = np.unique(values)
                if len(unique_values) <= 5:
                    for val in unique_values:
                        count = np.sum(np.array(values) == val)
                        percentage = (count / len(values)) * 100
                        print(f"    ç±»åˆ« {int(val)}: {count} æ ·æœ¬ ({percentage:.1f}%)")
    
    # éªŒè¯é›†ç»Ÿè®¡
    if val_items:
        print("éªŒè¯é›†:")
        for i, col in enumerate(label_columns):
            values = [item['labels'][i] for item in val_items]
            print(f"  {col}:")
            print(f"    å‡å€¼: {np.mean(values):.2f}")
            print(f"    æ ‡å‡†å·®: {np.std(values):.2f}")
            print(f"    èŒƒå›´: [{np.min(values):.2f}, {np.max(values):.2f}]")
            
            # å¦‚æœæ˜¯åˆ†ç±»æ ‡ç­¾ï¼Œæ˜¾ç¤ºåˆ†å¸ƒ
            if 'AS' in col.upper():
                unique_values = np.unique(values)
                if len(unique_values) <= 5:
                    for val in unique_values:
                        count = np.sum(np.array(values) == val)
                        percentage = (count / len(values)) * 100
                        print(f"    ç±»åˆ« {int(val)}: {count} æ ·æœ¬ ({percentage:.1f}%)")


# ä¸ºäº†å‘åå…¼å®¹ï¼Œæä¾›åˆ«å
create_hybrid_data_loaders = create_hybrid_dataloaders_from_config 