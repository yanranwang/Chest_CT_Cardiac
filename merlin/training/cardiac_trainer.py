import os
import json
import time
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    TENSORBOARD_AVAILABLE = False
    SummaryWriter = None
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
import monai
from monai.losses import DiceLoss
from monai.transforms import Compose
import logging
from pathlib import Path
from tqdm import tqdm
import warnings

# TODO: Translate 'æŠ‘åˆ¶'notå¿…è¦çš„warning
warnings.filterwarnings('ignore', category=UserWarning)

from merlin.models.cardiac_regression import CardiacFunctionModel, CardiacMetricsCalculator
from merlin.data.dataloaders import CTPersistentDataset
from merlin.data.monai_transforms import ImageTransforms


class CardiacDataset(Dataset):
    """å¿ƒè„åŠŸèƒ½æ•°æ®é›† - æ”¯æŒä»CSVæ–‡ä»¶Readæ•°æ®"""
    def __init__(self, data_list, transform=None, cardiac_metric_columns=None):
        self.data_list = data_list
        self.transform = transform or ImageTransforms
        self.cardiac_metric_columns = cardiac_metric_columns or []
        self.metric_names = CardiacMetricsCalculator.get_metric_names()
    
    def __len__(self):
        return len(self.data_list)
    
    def __getitem__(self, idx):
        sample = self.data_list[idx]
        
        # Loadandé¢„Processimage
        if self.transform:
            sample = self.transform(sample)
        
        # Getcardiac functionlabels
        if 'cardiac_metrics' in sample and sample['cardiac_metrics'] is not None:
            cardiac_metrics = torch.tensor(sample['cardiac_metrics'], dtype=torch.float32)
        else:
            # ifæ²¡hasçœŸå®labelsï¼ŒæŠ›å‡ºerrorè€Œnotisä½¿ç”¨æ¨¡æ‹Ÿdata
            raise ValueError(f"Sample {idx} (patient_id: {sample.get('patient_id', 'unknown')}) lacks cardiac function label dataã€‚"
                           f"è¯·ç¡®ä¿CSVæ–‡ä»¶ä¸­åŒ…å«æœ‰æ•ˆçš„å¿ƒè„åŠŸèƒ½æŒ‡æ ‡åˆ—ï¼Œæˆ–Checkæ•°æ®é¢„Processè¿‡ç¨‹ã€‚")
        
        # TODO: Translate 'ç¡®ä¿'labelsincludeLVEFandASä¸¤ä¸ªvalue
        if len(cardiac_metrics) < 2:
            # iflabelscountnotè¶³ï¼ŒæŠ›å‡ºerrorè€Œnotisä½¿ç”¨æ¨¡æ‹Ÿdata
            raise ValueError(f"Sample {idx} (patient_id: {sample.get('patient_id', 'unknown')}) çš„å¿ƒè„åŠŸèƒ½æ ‡ç­¾æ•°é‡ä¸è¶³ã€‚"
                           f"æœŸæœ›è‡³å°‘2ä¸ªæ ‡ç­¾(LVEFå’ŒAS)ï¼Œä½†åªæœ‰ {len(cardiac_metrics)} ä¸ªã€‚"
                           f"è¯·Checkcardiac_metric_columnsé…ç½®å’ŒCSVæ•°æ®ã€‚")
        
        return {
            'image': sample['image'],
            'cardiac_metrics': cardiac_metrics,
            'patient_id': sample.get('patient_id', f'patient_{idx}'),
            'basename': sample.get('basename', f'unknown_{idx}'),
            'folder': sample.get('folder', 'unknown'),
            'image_path': sample.get('image', ''),
            'metadata': sample.get('metadata', {})
        }
    
class CardiacTrainer:
    """Cardiac function regression trainer"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device(config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))
        
        # Create output directory
        self.output_dir = Path(config['output_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Setup logging (must be before using logger)
        self._setup_logging()
        
        # Set random seed
        self._set_random_seed(config.get('seed', 42))
        
        # Initialize model
        self.model = self._build_model()
        
        # Initialize optimizer and scheduler
        self.optimizer = self._build_optimizer()
        self.scheduler = self._build_scheduler()
        
        # Initialize loss function
        self.criterion = self._build_loss_function()
        
        # Initialize training records
        self.best_val_loss = float('inf')
        self.train_losses = []
        self.val_losses = []
        self.epoch_times = []
        
        # Initialize tensorboard writer
        if config.get('use_tensorboard', True) and TENSORBOARD_AVAILABLE:
            self.writer = SummaryWriter(self.output_dir / 'tensorboard')
        else:
            self.writer = None
            if config.get('use_tensorboard', True) and not TENSORBOARD_AVAILABLE:
                self.logger.warning("TensorBoardä¸å¯ç”¨ï¼Œè·³è¿‡TensorBoardæ—¥å¿—è®°å½•")
    
    def _set_random_seed(self, seed):
        """Set random seed"""
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        np.random.seed(seed)
    
    def _build_model(self):
        """Build model"""
        model = CardiacFunctionModel(
            pretrained_model_path=self.config.get('pretrained_model_path')
        )
        
        # Whether to freeze image encoder
        if self.config.get('freeze_encoder', False):
            model.freeze_encoder(True)
            self.logger.info("Image encoder frozen")
        
        model = model.to(self.device)
        
        # Multi-GPU training
        if torch.cuda.device_count() > 1:
            # Check if batch_size is large enough to avoid batch_size=1 per GPU
            batch_size = self.config.get('batch_size', 4)
            num_gpus = torch.cuda.device_count()
            if batch_size < num_gpus:
                self.logger.warning(f"Batch size {batch_size} is smaller than GPU count {num_gpus}, may cause BatchNorm error")
                self.logger.warning("Consider increasing batch_size or reducing GPU count")
            
            model = nn.DataParallel(model)
            self.logger.info(f"Using {torch.cuda.device_count()} GPUs for training")
        
        return model
    
    def _build_optimizer(self):
        """Build optimizer"""
        optimizer_name = self.config.get('optimizer', 'adam').lower()
        lr = self.config.get('learning_rate', 1e-4)
        weight_decay = self.config.get('weight_decay', 1e-5)
        
        if optimizer_name == 'adam':
            optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)
        elif optimizer_name == 'adamw':
            optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)
        elif optimizer_name == 'sgd':
            optimizer = optim.SGD(self.model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.9)
        else:
            raise ValueError(f"Unsupported optimizer: {optimizer_name}")
        
        return optimizer
    
    def _build_scheduler(self):
        """Build learning rate scheduler"""
        scheduler_config = self.config.get('scheduler', {})
        scheduler_type = scheduler_config.get('type', 'cosine')
        
        if scheduler_type == 'cosine':
            scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, 
                T_max=self.config.get('epochs', 100),
                eta_min=scheduler_config.get('eta_min', 1e-6)
            )
        elif scheduler_type == 'step':
            scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=scheduler_config.get('step_size', 30),
                gamma=scheduler_config.get('gamma', 0.1)
            )
        elif scheduler_type == 'plateau':
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=scheduler_config.get('factor', 0.5),
                patience=scheduler_config.get('patience', 10)
            )
        else:
            scheduler = None
        
        return scheduler
    
    def _build_loss_function(self):
        """Build loss function"""
        from merlin.models.cardiac_regression import CardiacLoss
        
        # Use specialized cardiac function loss that handles LVEF regression and AS classification
        regression_weight = self.config.get('regression_weight', 1.0)
        classification_weight = self.config.get('classification_weight', 1.0)
        
        # Calculateclassåˆ«weightsï¼ˆifenableï¼‰
        class_weights = None
        if self.config.get('use_class_weights', False):
            class_weights = self._calculate_class_weights()
            if class_weights is not None:
                self.logger.info(f"ä½¿ç”¨Classæƒé‡: {class_weights.tolist()}")
        
        criterion = CardiacLoss(
            regression_weight=regression_weight,
            classification_weight=classification_weight,
            class_weights=class_weights
        )
        
        return criterion
    
    def _calculate_class_weights(self):
        """CalculateASåˆ†ç±»çš„Classæƒé‡"""
        try:
            if hasattr(self, 'train_loader') and self.train_loader is not None:
                # fromTraindataLoadå™¨ä¸­statisticslabelsåˆ†å¸ƒ
                as_labels = []
                for batch in self.train_loader:
                    if 'as_maybe' in batch:
                        as_labels.extend(batch['as_maybe'].cpu().numpy())
                    elif 'AS_maybe' in batch:
                        as_labels.extend(batch['AS_maybe'].cpu().numpy())
                    elif 'labels' in batch and len(batch['labels'].shape) > 1:
                        # TODO: Translate 'å‡è®¾'ASlabelsissecondåˆ—
                        as_labels.extend(batch['labels'][:, 1].cpu().numpy())
                
                if len(as_labels) > 0:
                    as_labels = np.array(as_labels)
                    unique, counts = np.unique(as_labels, return_counts=True)
                    total = len(as_labels)
                    
                    # Calculateweightsï¼štotal / (classåˆ«æ•° * æ¯classsampleæ•°)
                    weights = total / (len(unique) * counts)
                    
                    # Createweightså¼ é‡ï¼Œindexå¯¹åº”classåˆ«labels
                    class_weights = torch.zeros(2)  # TODO: Translate 'å‡è®¾åª'has0and1ä¸¤class
                    for i, label in enumerate(unique):
                        class_weights[int(label)] = weights[i]
                    
                    return class_weights
            
            return None
        except Exception as e:
            self.logger.warning(f"CalculateClassæƒé‡å¤±è´¥: {e}")
            return None
    
    def _print_label_distribution(self):
        """Printè®­ç»ƒå’ŒValidation setçš„æ ‡ç­¾åˆ†å¸ƒ"""
        try:
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“Š æ•°æ®é›†Label Distribution Statistics")
            self.logger.info("=" * 60)
            
            # statisticsTrainé›†
            if hasattr(self, 'train_loader') and self.train_loader is not None:
                train_stats = self._get_dataset_stats(self.train_loader, "Training set")
                
            # statisticsvalidateé›†
            if hasattr(self, 'val_loader') and self.val_loader is not None:
                val_stats = self._get_dataset_stats(self.val_loader, "Validation set")
                
            self.logger.info("=" * 60)
            
        except Exception as e:
            self.logger.warning(f"Statisticsæ ‡ç­¾åˆ†å¸ƒå¤±è´¥: {e}")
    
    def _get_dataset_stats(self, dataloader, dataset_name):
        """Getæ•°æ®é›†Statisticsinfo"""
        lvef_values = []
        as_labels = []
        
        # TODO: Translate 'ä¸´æ—¶'Setä¸ºEvaluatemodeä»¥é¿å…å½±å“Train
        original_training = self.model.training
        self.model.eval()
        
        try:
            with torch.no_grad():
                for i, batch in enumerate(dataloader):
                    # TODO: Translate 'åª'statisticså‰å‡ ä¸ªbatchä»¥åŠ faståº¦
                    if i >= 10:  # TODO: Translate 'æœ€å¤š'statistics10ä¸ªbatch
                        break
                        
                    if 'lvef' in batch:
                        lvef_values.extend(batch['lvef'].cpu().numpy())
                    if 'as_maybe' in batch:
                        as_labels.extend(batch['as_maybe'].cpu().numpy())
                    elif 'AS_maybe' in batch:
                        as_labels.extend(batch['AS_maybe'].cpu().numpy())
                    elif 'labels' in batch and len(batch['labels'].shape) > 1:
                        lvef_values.extend(batch['labels'][:, 0].cpu().numpy())
                        as_labels.extend(batch['labels'][:, 1].cpu().numpy())
        finally:
            # restoreåŸå§‹Trainmode
            self.model.train(original_training)
        
        # statisticsLVEF
        if len(lvef_values) > 0:
            lvef_values = np.array(lvef_values)
            self.logger.info(f"{dataset_name} LVEFStatistics:")
            self.logger.info(f"  Sampleæ•°: {len(lvef_values)}")
            self.logger.info(f"  å‡å€¼: {lvef_values.mean():.2f}")
            self.logger.info(f"  Standard deviation: {lvef_values.std():.2f}")
            self.logger.info(f"  Range: [{lvef_values.min():.2f}, {lvef_values.max():.2f}]")
        
        # statisticsASlabels
        if len(as_labels) > 0:
            as_labels = np.array(as_labels)
            unique, counts = np.unique(as_labels, return_counts=True)
            total = len(as_labels)
            
            self.logger.info(f"{dataset_name} ASåˆ†ç±»Statistics:")
            self.logger.info(f"  æ€»Sampleæ•°: {total}")
            for label, count in zip(unique, counts):
                percentage = (count / total) * 100
                self.logger.info(f"  Class {int(label)}: {count} Sample ({percentage:.1f}%)")
            
            # Calculateæ­£è´Ÿsampleæ¯”ä¾‹
            if len(unique) == 2:
                pos_count = counts[unique == 1][0] if 1 in unique else 0
                neg_count = counts[unique == 0][0] if 0 in unique else 0
                if neg_count > 0:
                    ratio = pos_count / neg_count
                    self.logger.info(f"  æ­£è´ŸSampleæ¯”ä¾‹: 1:{ratio:.2f}")
        
        return {'lvef_stats': lvef_values if len(lvef_values) > 0 else None,
                'as_stats': as_labels if len(as_labels) > 0 else None}
    
    def _setup_logging(self):
        """Setup logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.output_dir / 'training.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def _format_time(self, seconds):
        """Format time display"""
        if seconds < 60:
            return f"{seconds:.1f}s"
        elif seconds < 3600:
            minutes = seconds // 60
            secs = seconds % 60
            return f"{minutes:.0f}m {secs:.0f}s"
        else:
            hours = seconds // 3600
            minutes = (seconds % 3600) // 60
            return f"{hours:.0f}h {minutes:.0f}m"
    
    def train_epoch(self, train_loader, epoch):
        """Train one epoch"""
        self.model.train()
        epoch_loss = 0.0
        num_batches = len(train_loader)
        
        # Create progress bar
        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1:3d}/{self.config.get("epochs", 100)}', 
                   ncols=120, leave=False)
        
        batch_losses = []
        for batch_idx, batch in enumerate(pbar):
            images = batch['image'].to(self.device)
            
            # Extract LVEF and AS target values from cardiac_metrics
            cardiac_metrics = batch['cardiac_metrics'].to(self.device)
            # Assume first value is LVEF, second is AS
            if cardiac_metrics.shape[1] >= 2:
                lvef_targets = cardiac_metrics[:, 0]  # LVEF target
                as_targets = cardiac_metrics[:, 1]    # AS target
            else:
                # If only one value, assume it's LVEF, set AS to 0
                lvef_targets = cardiac_metrics[:, 0]
                as_targets = torch.zeros_like(lvef_targets)
            
            # Forward pass
            self.optimizer.zero_grad()
            lvef_preds, as_preds = self.model(images)
            
            # Calculate loss
            loss_dict = self.criterion(lvef_preds, as_preds, lvef_targets, as_targets)
            loss = loss_dict['total_loss']
            
            # Backward pass
            loss.backward()
            
            # Gradient clipping
            if self.config.get('grad_clip', 0) > 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['grad_clip'])
            
            self.optimizer.step()
            
            # Record loss
            batch_losses.append(loss.item())
            epoch_loss += loss.item()
            
            # Update progress bar
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'LR': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
            })
            
            # Log training progress
            if batch_idx % self.config.get('log_interval', 10) == 0:
                self.logger.info(
                    f'Epoch {epoch+1:3d}/{self.config.get("epochs", 100)} '
                    f'[{batch_idx:4d}/{num_batches:4d}] '
                    f'Loss: {loss.item():.6f} '
                    f'LR: {self.optimizer.param_groups[0]["lr"]:.2e}'
                )
        
        pbar.close()
        
        # Calculate average loss
        avg_loss = epoch_loss / num_batches
        self.train_losses.append(avg_loss)
        
        return avg_loss
    
    def validate_epoch(self, val_loader, epoch):
        """Validateä¸€ä¸ªepoch"""
        self.model.eval()
        epoch_loss = 0.0
        all_predictions = []
        all_targets = []
        
        # Createvalidateè¿›åº¦æ¡
        pbar = tqdm(val_loader, desc=f'Validating', ncols=120, leave=False)
        
        with torch.no_grad():
            for batch in pbar:
                images = batch['image'].to(self.device)
                
                # fromcardiac_metricsä¸­ExtractLVEFandASç›®æ ‡value
                cardiac_metrics = batch['cardiac_metrics'].to(self.device)
                if cardiac_metrics.shape[1] >= 2:
                    lvef_targets = cardiac_metrics[:, 0]  # LVEFç›®æ ‡value
                    as_targets = cardiac_metrics[:, 1]    # ASç›®æ ‡value
                else:
                    lvef_targets = cardiac_metrics[:, 0]
                    as_targets = torch.zeros_like(lvef_targets)
                
                # TODO: Translate 'å‰å‘ä¼ æ’­'lvef_preds, as_preds = self.model(images)
                
                # Calculateloss
                loss_dict = self.criterion(lvef_preds, as_preds, lvef_targets, as_targets)
                loss = loss_dict['total_loss']
                epoch_loss += loss.item()
                
                # updatevalidateè¿›åº¦æ¡
                pbar.set_postfix({'Val Loss': f'{loss.item():.4f}'})
                
                # TODO: Translate 'æ”¶é›†'PredictandçœŸvalueç”¨äºæŒ‡æ ‡Calculate
                predictions = torch.stack([lvef_preds.squeeze(), as_preds.squeeze()], dim=1)
                targets = torch.stack([lvef_targets, as_targets], dim=1)
                all_predictions.append(predictions.cpu().numpy())
                all_targets.append(targets.cpu().numpy())
        
        pbar.close()
        
        avg_loss = epoch_loss / len(val_loader)
        self.val_losses.append(avg_loss)
        
        # CalculateEvaluateæŒ‡æ ‡
        all_predictions = np.concatenate(all_predictions, axis=0)
        all_targets = np.concatenate(all_targets, axis=0)
        
        metrics = self._calculate_metrics(all_predictions, all_targets)
        
        # recordvalidateresults
        self.logger.info(f'Validateç»“æœ - Epoch {epoch+1:3d} | Loss: {avg_loss:.6f}')
        for metric_name, value in metrics.items():
            self.logger.info(f'  {metric_name}: {value:.4f}')
        
        if self.writer:
            self.writer.add_scalar('Val/Loss', avg_loss, epoch)
            for metric_name, value in metrics.items():
                self.writer.add_scalar(f'Val/{metric_name}', value, epoch)
        
        return avg_loss, metrics
    
    def _calculate_metrics(self, predictions, targets):
        """Calculateè¯„ä¼°æŒ‡æ ‡"""
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        
        metrics = {}
        
        # LVEFregressionæŒ‡æ ‡ï¼ˆç¬¬0åˆ—ï¼‰
        lvef_preds = predictions[:, 0]
        lvef_targets = targets[:, 0]
        
        metrics['LVEF_MSE'] = mean_squared_error(lvef_targets, lvef_preds)
        metrics['LVEF_MAE'] = mean_absolute_error(lvef_targets, lvef_preds)
        metrics['LVEF_R2'] = r2_score(lvef_targets, lvef_preds)
        
        # ASclassificationæŒ‡æ ‡ï¼ˆç¬¬1åˆ—ï¼‰
        as_preds = predictions[:, 1]
        as_targets = targets[:, 1]
        
        # TODO: Translate 'å°†æ¦‚ç‡'Convertä¸ºäºŒclassificationPredict
        as_pred_binary = (as_preds > 0.5).astype(int)
        as_targets_binary = as_targets.astype(int)
        
        metrics['AS_Accuracy'] = accuracy_score(as_targets_binary, as_pred_binary)
        metrics['AS_Precision'] = precision_score(as_targets_binary, as_pred_binary, zero_division=0)
        metrics['AS_Recall'] = recall_score(as_targets_binary, as_pred_binary, zero_division=0)
        metrics['AS_F1'] = f1_score(as_targets_binary, as_pred_binary, zero_division=0)
        
        # CalculateASçš„AUC
        try:
            from sklearn.metrics import roc_auc_score
            metrics['AS_AUC'] = roc_auc_score(as_targets_binary, as_preds)
        except ValueError:
            metrics['AS_AUC'] = 0.0
        
        return metrics
    
    def save_checkpoint(self, epoch, is_best=False):
        """SaveCheckç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'best_val_loss': self.best_val_loss,
            'config': self.config
        }
        
        # Saveæœ€æ–°Checkpoint
        torch.save(checkpoint, self.output_dir / 'checkpoint_latest.pth')
        
        # Saveæœ€ä½³model
        if is_best:
            torch.save(checkpoint, self.output_dir / 'checkpoint_best.pth')
            torch.save(self.model.state_dict(), self.output_dir / 'best_model.pth')
            self.logger.info(f'ğŸ’¾ Saveæœ€ä½³æ¨¡å‹ (Epoch {epoch+1}, Val Loss: {self.best_val_loss:.6f})')
    
    def load_checkpoint(self, checkpoint_path):
        """LoadCheckç‚¹"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if self.scheduler and checkpoint['scheduler_state_dict']:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.train_losses = checkpoint.get('train_losses', [])
        self.val_losses = checkpoint.get('val_losses', [])
        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        
        self.logger.info(f'ä» {checkpoint_path} LoadCheckç‚¹')
        return checkpoint['epoch']
    
    def train(self, train_loader, val_loader=None, start_epoch=0):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        epochs = self.config.get('epochs', 100)
        
        # TODO: Translate 'å­˜å‚¨'dataLoadå™¨ä¾›å…¶ä»–methodä½¿ç”¨
        self.train_loader = train_loader
        self.val_loader = val_loader
        
        # PrintTrainstartinfo
        print("=" * 80)
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒå¿ƒè„åŠŸèƒ½é¢„æµ‹æ¨¡å‹")
        print("=" * 80)
        print(f"ğŸ“Š è®­ç»ƒå‚æ•°:")
        print(f"   æ€»è½®æ•°: {epochs}")
        print(f"   æ‰¹é‡å¤§å°: {self.config.get('batch_size', 4)}")
        print(f"   å­¦ä¹ ç‡: {self.config.get('learning_rate', 1e-4)}")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   ä¼˜åŒ–å™¨: {self.config.get('optimizer', 'adam')}")
        print(f"   è°ƒåº¦å™¨: {self.config.get('scheduler', {}).get('type', 'None')}")
        print(f"ğŸ“ æ•°æ®Statistics:")
        print(f"   Training setå¤§å°: {len(train_loader.dataset)}")
        if val_loader:
            print(f"   Validation setå¤§å°: {len(val_loader.dataset)}")
        print(f"   æ¯è½®æ‰¹æ¬¡æ•°: {len(train_loader)}")
        print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {self.output_dir}")
        if self.writer:
            print(f"ğŸ“ˆ TensorBoard: {self.output_dir / 'tensorboard'}")
        print("=" * 80)
        
        self.logger.info(f'å¼€å§‹è®­ç»ƒï¼Œå…± {epochs} ä¸ªepoch')
        
        # Printlabelsåˆ†å¸ƒstatistics
        self._print_label_distribution()
        
        # Trainstarttime
        training_start_time = time.time()
        
        for epoch in range(start_epoch, epochs):
            epoch_start_time = time.time()
            
            # Printepochstartinfo
            print(f"\nğŸ”„ Epoch {epoch+1}/{epochs}")
            print("-" * 50)
            
            # Train
            train_loss = self.train_epoch(train_loader, epoch)
            
            # validate
            val_loss, val_metrics = None, {}
            if val_loader:
                val_loss, val_metrics = self.validate_epoch(val_loader, epoch)
                
                # updatelearning rateè°ƒåº¦å™¨
                if self.scheduler:
                    if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                        self.scheduler.step(val_loss)
                    else:
                        self.scheduler.step()
                
                # Checkiså¦ä¸ºæœ€ä½³model
                is_best = val_loss < self.best_val_loss
                if is_best:
                    improvement = self.best_val_loss - val_loss
                    self.best_val_loss = val_loss
                    print(f"âœ¨ æ–°çš„æœ€ä½³æ¨¡å‹ï¼ValidateæŸå¤±é™ä½äº† {improvement:.6f}")
                
                # SaveCheckpoint
                if epoch % self.config.get('save_interval', 10) == 0 or is_best:
                    self.save_checkpoint(epoch, is_best)
            else:
                # TODO: Translate 'æ— 'validateé›†æ—¶
                if self.scheduler and not isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step()
                
                if epoch % self.config.get('save_interval', 10) == 0:
                    self.save_checkpoint(epoch)
            
            # Calculatetime
            epoch_time = time.time() - epoch_start_time
            self.epoch_times.append(epoch_time)
            
            # TODO: Translate 'é¢„ä¼°å‰©ä½™'time
            avg_epoch_time = np.mean(self.epoch_times[-10:])  # TODO: Translate 'ä½¿ç”¨æœ€è¿‘10ä¸ª'epochçš„averagetime
            remaining_epochs = epochs - epoch - 1
            estimated_remaining_time = avg_epoch_time * remaining_epochs
            
            # Printepochæ€»ç»“
            print(f"ğŸ“Š Epoch {epoch+1} æ€»ç»“:")
            print(f"   è®­ç»ƒæŸå¤±: {train_loss:.6f}")
            if val_loss is not None:
                print(f"   ValidateæŸå¤±: {val_loss:.6f}")
                print(f"   LVEF RÂ²: {val_metrics.get('LVEF_R2', 0):.4f}")
                print(f"   AS å‡†ç¡®ç‡: {val_metrics.get('AS_Accuracy', 0):.4f}")
            print(f"   è½®æ¬¡è€—æ—¶: {self._format_time(epoch_time)}")
            print(f"   å¹³å‡è€—æ—¶: {self._format_time(avg_epoch_time)}")
            print(f"   é¢„ä¼°å‰©ä½™: {self._format_time(estimated_remaining_time)}")
            print(f"   å½“å‰å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.2e}")
            
            # recordtotensorboard
            if self.writer:
                self.writer.add_scalar('Train/EpochLoss', train_loss, epoch)
                self.writer.add_scalar('Train/EpochTime', epoch_time, epoch)
                if val_loader:
                    self.writer.add_scalar('Val/EpochLoss', val_loss, epoch)
        
        # Traincomplete
        total_training_time = time.time() - training_start_time
        
        print("\n" + "=" * 80)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("=" * 80)
        print(f"ğŸ“ˆ è®­ç»ƒStatistics:")
        print(f"   æ€»è®­ç»ƒæ—¶é—´: {self._format_time(total_training_time)}")
        print(f"   å¹³å‡æ¯è½®æ—¶é—´: {self._format_time(np.mean(self.epoch_times))}")
        print(f"   æœ€ä½³ValidateæŸå¤±: {self.best_val_loss:.6f}")
        print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶:")
        print(f"   æœ€ä½³æ¨¡å‹: {self.output_dir}/best_model.pth")
        print(f"   è®­ç»ƒæ—¥å¿—: {self.output_dir}/training.log")
        print(f"   é…ç½®æ–‡ä»¶: {self.output_dir}/config.json")
        if self.writer:
            print(f"   TensorBoard: {self.output_dir}/tensorboard")
        print("=" * 80)
        
        self.logger.info('è®­ç»ƒå®Œæˆï¼')
        
        # Saveæœ€ç»ˆmodel
        self.save_checkpoint(epochs - 1)
        
        # SaveTrainconfig
        with open(self.output_dir / 'config.json', 'w') as f:
            json.dump(self.config, f, indent=2)
        
        if self.writer:
            self.writer.close()


def load_and_validate_csv_data(config):
    """Loadå’ŒValidateCSVæ•°æ®"""
    csv_path = config.get('csv_path')
    if not csv_path or not os.path.exists(csv_path):
        raise FileNotFoundError(f"CSV file does not exist: {csv_path}")
    
    print(f"ä» {csv_path} Readæ•°æ®...")
    df = pd.read_csv(csv_path)
    print(f"åŸå§‹æ•°æ®é›†å¤§å°: {len(df)} è¡Œ")
    
    # Checkå¿…éœ€çš„åˆ—
    required_columns = config.get('required_columns', ['basename', 'folder'])
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise ValueError(f"CSVæ–‡ä»¶ä¸­Missing required columns: {missing_columns}")
    
    # Checkcardiac functionæŒ‡æ ‡åˆ—
    cardiac_metric_columns = config.get('cardiac_metric_columns', [])
    if not cardiac_metric_columns:
        raise ValueError("é…ç½®ä¸­å¿…é¡»æŒ‡å®šcardiac_metric_columnsï¼Œä¸èƒ½ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œè®­ç»ƒã€‚"
                        "è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®cardiac_metric_columnsï¼Œä¾‹å¦‚: ['lvef', 'as_maybe']")
    
    missing_cardiac_columns = [col for col in cardiac_metric_columns if col not in df.columns]
    if missing_cardiac_columns:
        raise ValueError(f"CSVæ–‡ä»¶ä¸­ç¼ºå°‘å¿…éœ€çš„å¿ƒè„åŠŸèƒ½æŒ‡æ ‡åˆ—: {missing_cardiac_columns}ã€‚"
                        f"å¯ç”¨çš„åˆ—: {list(df.columns)}")
    
    print(f"æ‰¾åˆ°å¿ƒè„åŠŸèƒ½æŒ‡æ ‡åˆ—: {cardiac_metric_columns}")
    
    # Checkiså¦è‡³å°‘éœ€è¦2ä¸ªcardiac functionæŒ‡æ ‡ï¼ˆLVEFandASï¼‰
    if len(cardiac_metric_columns) < 2:
        raise ValueError(f"è‡³å°‘éœ€è¦2ä¸ªå¿ƒè„åŠŸèƒ½æŒ‡æ ‡åˆ—ï¼ˆLVEFå’ŒASï¼‰ï¼Œä½†åªæä¾›äº† {len(cardiac_metric_columns)} ä¸ª: {cardiac_metric_columns}")
    
    # dataClean
    if config.get('remove_missing_files', True):
        initial_count = len(df)
        df = df.dropna(subset=['basename', 'folder'])
        df = df[df['basename'].notna() & df['folder'].notna()]
        if len(df) < initial_count:
            print(f"Removeäº† {initial_count - len(df)} è¡Œç¼ºå¤±basenameæˆ–folderçš„æ•°æ®")
    
    # Removeé‡å¤é¡¹
    if config.get('remove_duplicates', True):
        initial_count = len(df)
        df = df.drop_duplicates(subset=['basename', 'folder'])
        if len(df) < initial_count:
            print(f"Removeäº† {initial_count - len(df)} è¡Œé‡å¤æ•°æ®")
    
    print(f"Cleanåæ•°æ®é›†å¤§å°: {len(df)} è¡Œ")
    return df, cardiac_metric_columns


def build_data_list(df, config, cardiac_metric_columns):
    """ä»DataFrameBuildæ•°æ®åˆ—è¡¨"""
    base_path = config.get('base_path', '/dataNAS/data/ct_data/ct_scans')
    data_list = []
    missing_files = []
    
    for idx, row in df.iterrows():
        basename = row['basename']
        folder = row['folder']
        
        # Buildfilepath
        image_path_template = config.get('image_path_template', '{base_path}/stanford_{folder}/{basename}.nii.gz')
        image_path = image_path_template.format(
            base_path=base_path,
            folder=folder,
            basename=basename
        )
        
        # Checkfileiså¦å­˜in
        if config.get('check_file_exists', False):
            if not os.path.exists(image_path):
                missing_files.append(image_path)
                continue
        
        # Getcardiac functionæŒ‡æ ‡data
        cardiac_metrics = None
        try:
            cardiac_metrics = []
            for col in cardiac_metric_columns:
                value = row[col]
                if pd.isna(value):
                    print(f"è­¦å‘Š: è¡Œ {idx} (basename: {basename}) çš„åˆ— '{col}' ç¼ºå°‘æ•°æ®ï¼Œè·³è¿‡è¯¥Sample")
                    cardiac_metrics = None
                    break
                cardiac_metrics.append(float(value))
            
            if cardiac_metrics is not None:
                cardiac_metrics = np.array(cardiac_metrics, dtype=np.float32)
        except (ValueError, TypeError) as e:
            print(f"è­¦å‘Š: è¡Œ {idx} (basename: {basename}) çš„å¿ƒè„åŠŸèƒ½æŒ‡æ ‡æ•°æ®æ— æ•ˆ: {e}ï¼Œè·³è¿‡è¯¥Sample")
            cardiac_metrics = None
        
        # ifæ²¡hashasæ•ˆçš„cardiac functionæŒ‡æ ‡dataï¼Œskipè¿™ä¸ªsample
        if cardiac_metrics is None:
            continue
        
        # TODO: Translate 'æ”¶é›†å…¶ä»–å…ƒ'data
        metadata = {}
        metadata_columns = config.get('metadata_columns', [])
        for col in metadata_columns:
            if col in row and pd.notna(row[col]):
                metadata[col] = row[col]
        
        data_item = {
            'image': image_path,
            'cardiac_metrics': cardiac_metrics,
            'patient_id': row.get('patient_id', basename),
            'basename': basename,
            'folder': folder,
            'metadata': metadata
        }
        
        data_list.append(data_item)
    
    print(f"æˆåŠŸBuild {len(data_list)} ä¸ªæ•°æ®é¡¹")
    
    if missing_files and config.get('check_file_exists', False):
        print(f"è­¦å‘Š: æœ‰ {len(missing_files)} ä¸ªfile does not exist")
        if len(missing_files) <= 5:
            print("ç¼ºå¤±çš„æ–‡ä»¶:")
            for f in missing_files:
                print(f"  {f}")
        else:
            print(f"å‰5ä¸ªç¼ºå¤±çš„æ–‡ä»¶:")
            for f in missing_files[:5]:
                print(f"  {f}")
    
    return data_list


def split_data(data_list, config):
    """Splitæ•°æ®ä¸ºTraining setå’ŒValidation set"""
    split_method = config.get('split_method', 'random')
    split_ratio = config.get('train_val_split', 0.8)
    random_state = config.get('seed', 42)
    
    if split_method == 'random':
        # randomSplit
        train_data, val_data = train_test_split(
            data_list, 
            train_size=split_ratio, 
            random_state=random_state,
            shuffle=True
        )
    elif split_method == 'sequential':
        # sequenceSplit
        split_idx = int(len(data_list) * split_ratio)
        train_data = data_list[:split_idx]
        val_data = data_list[split_idx:]
    elif split_method == 'patient_based':
        # TODO: Translate 'åŸºäºæ‚£è€…'IDçš„Splitï¼ˆé¿å…åŒä¸€æ‚£è€…çš„dataå‡ºç°inTrainandvalidateé›†ä¸­ï¼‰
        patient_ids = list(set([item['patient_id'] for item in data_list]))
        train_patients, val_patients = train_test_split(
            patient_ids,
            train_size=split_ratio,
            random_state=random_state,
            shuffle=True
        )
        
        train_data = [item for item in data_list if item['patient_id'] in train_patients]
        val_data = [item for item in data_list if item['patient_id'] in val_patients]
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®Split method: {split_method}")
    
    print(f"æ•°æ®Splitå®Œæˆ:")
    print(f"  Training set: {len(train_data)} samples")
    print(f"  Validation set: {len(val_data)} samples")
    
    return train_data, val_data


def create_data_loaders(config):
    """Createæ•°æ®Loadå™¨"""
    # LoadandvalidateCSVdata
    df, cardiac_metric_columns = load_and_validate_csv_data(config)
    
    # Builddataåˆ—è¡¨
    data_list = build_data_list(df, config, cardiac_metric_columns)
    
    if not data_list:
        raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®é¡¹")
    
    # Splitdata
    train_data, val_data = split_data(data_list, config)
    
    # Createdataé›†
    train_dataset = CardiacDataset(
        train_data, 
        cardiac_metric_columns=cardiac_metric_columns
    )
    
    val_dataset = None
    if val_data:
        val_dataset = CardiacDataset(
            val_data, 
            cardiac_metric_columns=cardiac_metric_columns
        )
    
    # CreatedataLoadå™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.get('batch_size', 4),
        shuffle=True,
        num_workers=config.get('num_workers', 4),
        pin_memory=True,
        drop_last=True  # Trainæ—¶å¿…é¡»Setä¸ºTrueï¼Œé¿å…BatchNormerror
    )
    
    val_loader = None
    if val_dataset:
        val_loader = DataLoader(
            val_dataset,
            batch_size=config.get('batch_size', 4),
            shuffle=False,
            num_workers=config.get('num_workers', 4),
            pin_memory=True,
            drop_last=False
        )
    
    # Savedatastatisticsinfo
    data_info = {
        'total_samples': len(data_list),
        'train_samples': len(train_data),
        'val_samples': len(val_data) if val_data else 0,
        'cardiac_metric_columns': cardiac_metric_columns,
        'split_method': config.get('split_method', 'random'),
        'split_ratio': config.get('train_val_split', 0.8)
    }
    
    # SavetoOutputç›®å½•
    output_dir = Path(config['output_dir'])
    output_dir.mkdir(parents=True, exist_ok=True)
    with open(output_dir / 'data_info.json', 'w', encoding='utf-8') as f:
        json.dump(data_info, f, indent=2, ensure_ascii=False)
    
    return train_loader, val_loader

