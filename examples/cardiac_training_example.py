#!/usr/bin/env python3
"""
å¿ƒè„åŠŸèƒ½å›å½’è®­ç»ƒç¤ºä¾‹

è¯¥è„šæœ¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨Merlinè¿›è¡Œå®Œæ•´çš„å¿ƒè„åŠŸèƒ½å›å½’è®­ç»ƒï¼š
1. ä»CSVæ–‡ä»¶åŠ è½½å¿ƒè„åŠŸèƒ½æ•°æ®
2. é…ç½®è®­ç»ƒå‚æ•°
3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
4. è®­ç»ƒå¿ƒè„åŠŸèƒ½é¢„æµ‹æ¨¡å‹
5. ä¿å­˜è®­ç»ƒç»“æœå’Œæ¨¡å‹

ä½¿ç”¨æ–¹æ³•:
    python cardiac_training_example.py
    
æˆ–è‡ªå®šä¹‰é…ç½®:
    python cardiac_training_example.py --config my_config.json
"""

import os
import sys
import json
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from merlin.training.cardiac_trainer import CardiacTrainer, create_data_loaders


def create_default_config():
    """åˆ›å»ºé»˜è®¤è®­ç»ƒé…ç½®"""
    config = {
        'output_dir': 'outputs/cardiac_training',
        'pretrained_model_path': '/dataNAS/people/joycewyr/Merlin/merlin/models/checkpoints/i3_resnet_clinical_longformer_best_clip_04-02-2024_23-21-36_epoch_99.pt',  # ä½¿ç”¨Merliné¢„è®­ç»ƒæ¨¡å‹æƒé‡
        'num_cardiac_metrics': 2,  # LVEFå›å½’ + ASåˆ†ç±»
        'epochs': 100,
        'batch_size': 4,
        'learning_rate': 1e-4,
        'weight_decay': 1e-5,
        'optimizer': 'adamw',
        'loss_function': 'mse',
        'scheduler': {
            'type': 'cosine',
            'eta_min': 1e-6
        },
        'freeze_encoder': True,  # é‡è¦ï¼šå†»ç»“é¢„è®­ç»ƒç¼–ç å™¨è¿›è¡Œå¾®è°ƒ
        'grad_clip': 1.0,
        'device': 'cuda',
        'seed': 42,
        'num_workers': 0,
        'log_interval': 5,  # æ›´é¢‘ç¹çš„æ—¥å¿—è¾“å‡ºï¼ˆæ¯5ä¸ªbatchï¼‰
        'save_interval': 5,  # æ›´é¢‘ç¹çš„æ¨¡å‹ä¿å­˜ï¼ˆæ¯5ä¸ªepochï¼‰
        'use_tensorboard': True,
        'drop_last': True,  # è®­ç»ƒæ—¶å¿…é¡»è®¾ç½®ä¸ºTrueï¼Œé¿å…BatchNormé”™è¯¯
        
        # è¿›åº¦ç›‘æ§å¢å¼ºé…ç½®
        'progress_bar': True,  # å¯ç”¨è¿›åº¦æ¡
        'show_gpu_memory': True,  # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨
        'show_eta': True,  # æ˜¾ç¤ºé¢„ä¼°å‰©ä½™æ—¶é—´
        'detailed_metrics': True,  # æ˜¾ç¤ºè¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡
        
        # æ•°æ®åˆ†å‰²é…ç½®
        'train_val_split': 0.8,
        'split_method': 'random',  # 'random', 'sequential', 'patient_based'
        
        # CSVæ•°æ®é…ç½®
        'csv_path': '/dataNAS/people/joycewyr/Merlin/filtered_echo_chestCT_data_filtered_chest_data.csv',
        'required_columns': ['basename', 'folder'],
        'cardiac_metric_columns': [],  # è®¾ç½®ä¸ºCSVä¸­åŒ…å«å¿ƒè„åŠŸèƒ½æŒ‡æ ‡çš„åˆ—ååˆ—è¡¨
        'metadata_columns': ['patient_id'],  # è¦ä¿å­˜çš„é¢å¤–å…ƒæ•°æ®åˆ—
        
        # æ–‡ä»¶è·¯å¾„é…ç½®
        'base_path': '/dataNAS/data/ct_data/ct_scans',
        'image_path_template': '{base_path}/stanford_{folder}/{basename}.nii.gz',
        'check_file_exists': False,  # è®¾ç½®ä¸ºTrueä»¥æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        
        # æ•°æ®æ¸…ç†é…ç½®
        'remove_missing_files': True,
        'remove_duplicates': True,
        
        # å¿«é€Ÿæ•°æ®åŠ è½½å™¨é…ç½®
        'use_fast_loader': False,  # è®¾ç½®ä¸ºTrueä»¥å¯ç”¨å¿«é€Ÿæ•°æ®åŠ è½½å™¨
        'preprocessed_data_dir': 'outputs/preprocessed_data',  # é¢„å¤„ç†æ•°æ®ç›®å½•
        'preprocess_batch_size': 16,  # é¢„å¤„ç†æ‰¹æ¬¡å¤§å°
        'cache_config': {
            'enable_cache': True,      # å¯ç”¨å†…å­˜ç¼“å­˜
            'cache_size': 1000,        # ç¼“å­˜å¤§å°
            'preload_train_data': False,  # æ˜¯å¦é¢„åŠ è½½è®­ç»ƒæ•°æ®åˆ°å†…å­˜
            'preload_val_data': False,    # æ˜¯å¦é¢„åŠ è½½éªŒè¯æ•°æ®åˆ°å†…å­˜
        }
    }
    return config


def load_config_from_file(config_path):
    """ä»JSONæ–‡ä»¶åŠ è½½é…ç½®"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    return config


def save_config(config, output_path):
    """ä¿å­˜é…ç½®åˆ°JSONæ–‡ä»¶"""
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)


def print_training_info(config):
    """æ‰“å°è®­ç»ƒé…ç½®ä¿¡æ¯"""
    print("=" * 80)
    print("ğŸ”§ è®­ç»ƒé…ç½®ä¿¡æ¯")
    print("=" * 80)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {config['output_dir']}")
    print(f"ğŸ“Š CSVæ–‡ä»¶: {config['csv_path']}")
    print(f"ğŸ¥ æ•°æ®è·¯å¾„: {config['base_path']}")
    print(f"ğŸ¯ è®­ç»ƒè½®æ•°: {config['epochs']}")
    print(f"ğŸ“¦ æ‰¹é‡å¤§å°: {config['batch_size']}")
    print(f"ğŸ“ å­¦ä¹ ç‡: {config['learning_rate']}")
    print(f"ğŸ”§ ä¼˜åŒ–å™¨: {config['optimizer']}")
    print(f"ğŸ’¾ æ—¥å¿—é—´éš”: {config['log_interval']} batches")
    print(f"ğŸ’¾ ä¿å­˜é—´éš”: {config['save_interval']} epochs")
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {config['device']}")
    print(f"ğŸ“ˆ TensorBoard: {'âœ…' if config['use_tensorboard'] else 'âŒ'}")
    print(f"ğŸ¯ æ•°æ®åˆ†å‰²: {config['train_val_split']:.1%} è®­ç»ƒ / {1-config['train_val_split']:.1%} éªŒè¯")
    print("=" * 80)


def check_dependencies():
    """æ£€æŸ¥è®­ç»ƒæ‰€éœ€çš„ä¾èµ–"""
    print("ğŸ” æ£€æŸ¥è®­ç»ƒä¾èµ–...")
    
    missing_deps = []
    
    try:
        import torch
        print(f"âœ… PyTorch {torch.__version__}")
        if torch.cuda.is_available():
            print(f"âœ… CUDA å¯ç”¨, {torch.cuda.device_count()} GPU(s)")
            for i in range(torch.cuda.device_count()):
                print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
        else:
            print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
    except ImportError:
        missing_deps.append("torch")
    
    try:
        import tqdm
        print(f"âœ… tqdm (è¿›åº¦æ¡)")
    except ImportError:
        missing_deps.append("tqdm")
        print("âŒ tqdm æœªå®‰è£…ï¼Œè¿›åº¦æ¡å°†ä¸å¯ç”¨")
    
    try:
        import tensorboard
        print(f"âœ… TensorBoard")
    except ImportError:
        missing_deps.append("tensorboard")
        print("âŒ TensorBoard æœªå®‰è£…ï¼Œè®­ç»ƒå¯è§†åŒ–å°†ä¸å¯ç”¨")
    
    try:
        import monai
        print(f"âœ… MONAI")
    except ImportError:
        missing_deps.append("monai")
    
    if missing_deps:
        print(f"\nâš ï¸  ç¼ºå°‘ä¾èµ–: {', '.join(missing_deps)}")
        print("è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
        print(f"pip install {' '.join(missing_deps)}")
        return False
    
    print("âœ… æ‰€æœ‰ä¾èµ–å·²å®‰è£…")
    return True


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¿ƒè„åŠŸèƒ½å›å½’è®­ç»ƒç¤ºä¾‹')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)')
    parser.add_argument('--output_dir', type=str, help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--csv_path', type=str, help='CSVæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--epochs', type=int, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, help='æ‰¹é‡å¤§å°')
    parser.add_argument('--learning_rate', type=float, help='å­¦ä¹ ç‡')
    parser.add_argument('--log_interval', type=int, help='æ—¥å¿—è¾“å‡ºé—´éš”')
    parser.add_argument('--save_interval', type=int, help='æ¨¡å‹ä¿å­˜é—´éš”')
    parser.add_argument('--device', type=str, choices=['cuda', 'cpu', 'auto'], help='è®­ç»ƒè®¾å¤‡')
    parser.add_argument('--use_pretrained', type=bool, default=True, help='æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡')
    parser.add_argument('--use_fast_loader', action='store_true', help='ä½¿ç”¨å¿«é€Ÿæ•°æ®åŠ è½½å™¨')
    parser.add_argument('--preprocessed_data_dir', type=str, help='é¢„å¤„ç†æ•°æ®ç›®å½•')
    parser.add_argument('--preprocess_batch_size', type=int, help='é¢„å¤„ç†æ‰¹æ¬¡å¤§å°')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        print("âŒ ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œè¯·å…ˆå®‰è£…ç¼ºå°‘çš„ä¾èµ–åŒ…")
        return
    
    # åŠ è½½é…ç½®
    if args.config:
        print(f"ğŸ“‹ ä»æ–‡ä»¶åŠ è½½é…ç½®: {args.config}")
        config = load_config_from_file(args.config)
    else:
        print("ğŸ“‹ ä½¿ç”¨é»˜è®¤é…ç½®")
        config = create_default_config()
    
            # éªŒè¯å’Œå¤„ç†Merliné¢„è®­ç»ƒæƒé‡
        if args.use_pretrained and config.get('pretrained_model_path'):
            print("\nğŸ” æ£€æŸ¥Merliné¢„è®­ç»ƒæƒé‡...")
            pretrained_path = config['pretrained_model_path']
            
            # å¦‚æœæƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•è‡ªåŠ¨ä¸‹è½½
            if not os.path.exists(pretrained_path):
                print(f"âŒ é¢„è®­ç»ƒæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {pretrained_path}")
                print("ğŸ”„ å°è¯•è‡ªåŠ¨ä¸‹è½½Merliné¢„è®­ç»ƒæƒé‡...")
                
                try:
                    # ä½¿ç”¨Merlinå†…ç½®çš„æƒé‡ä¸‹è½½åŠŸèƒ½
                    from merlin import Merlin
                    merlin_model = Merlin()  # è¿™ä¼šè‡ªåŠ¨ä¸‹è½½æƒé‡
                    
                    # è·å–å®é™…çš„æƒé‡è·¯å¾„
                    actual_checkpoint_path = os.path.join(
                        merlin_model.current_path, 
                        'checkpoints', 
                        merlin_model.checkpoint_name
                    )
                    
                    if os.path.exists(actual_checkpoint_path):
                        config['pretrained_model_path'] = actual_checkpoint_path
                        print(f"âœ… æˆåŠŸä¸‹è½½å¹¶è®¾ç½®é¢„è®­ç»ƒæƒé‡: {actual_checkpoint_path}")
                    else:
                        print("âŒ è‡ªåŠ¨ä¸‹è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
                        config['pretrained_model_path'] = None
                        
                except Exception as e:
                    print(f"âŒ è‡ªåŠ¨ä¸‹è½½æƒé‡å¤±è´¥: {e}")
                    print("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡ç»§ç»­è®­ç»ƒ")
                    config['pretrained_model_path'] = None
            else:
                print(f"âœ… æ‰¾åˆ°é¢„è®­ç»ƒæƒé‡æ–‡ä»¶: {pretrained_path}")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¿«é€Ÿæ•°æ®åŠ è½½å™¨
        use_fast_loader = config.get('use_fast_loader', False)
        if use_fast_loader:
            print("\nğŸš€ ä½¿ç”¨å¿«é€Ÿæ•°æ®åŠ è½½å™¨æ¨¡å¼")
            preprocessed_data_dir = config.get('preprocessed_data_dir')
            if not preprocessed_data_dir:
                print("âŒ ä½¿ç”¨å¿«é€Ÿæ•°æ®åŠ è½½å™¨éœ€è¦è®¾ç½® preprocessed_data_dir")
                print("è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬:")
                print("  python -m merlin.training.data_preprocessor --config config.json")
                return
            
            # æ£€æŸ¥é¢„å¤„ç†æ•°æ®æ–‡ä»¶
            hdf5_path = Path(preprocessed_data_dir) / 'preprocessed_data.h5'
            metadata_path = Path(preprocessed_data_dir) / 'data_metadata.json'
            
            if not hdf5_path.exists():
                print(f"âŒ é¢„å¤„ç†æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {hdf5_path}")
                print("è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬:")
                print("  python -m merlin.training.data_preprocessor --config config.json")
                return
            
            if not metadata_path.exists():
                print(f"âŒ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_path}")
                print("è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬:")
                print("  python -m merlin.training.data_preprocessor --config config.json")
                return
            
            print(f"âœ… æ‰¾åˆ°é¢„å¤„ç†æ•°æ®: {hdf5_path}")
            print(f"âœ… æ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶: {metadata_path}")
        else:
            print("\nğŸ“ ä½¿ç”¨æ ‡å‡†æ•°æ®åŠ è½½å™¨æ¨¡å¼")
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if args.output_dir:
        config['output_dir'] = args.output_dir
    if args.csv_path:
        config['csv_path'] = args.csv_path
    if args.epochs:
        config['epochs'] = args.epochs
    if args.batch_size:
        config['batch_size'] = args.batch_size
    if args.learning_rate:
        config['learning_rate'] = args.learning_rate
    if args.log_interval:
        config['log_interval'] = args.log_interval
    if args.save_interval:
        config['save_interval'] = args.save_interval
    if args.device:
        config['device'] = args.device
    if args.use_fast_loader:
        config['use_fast_loader'] = True
    if args.preprocessed_data_dir:
        config['preprocessed_data_dir'] = args.preprocessed_data_dir
    if args.preprocess_batch_size:
        config['preprocess_batch_size'] = args.preprocess_batch_size
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print_training_info(config)
    
    # ä¿å­˜é…ç½®
    output_dir = Path(config['output_dir'])
    config_save_path = output_dir / 'config.json'
    save_config(config, config_save_path)
    print(f"ğŸ“ é…ç½®å·²ä¿å­˜åˆ°: {config_save_path}")
    
    try:
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print("\nğŸ“‚ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        use_fast_loader = config.get('use_fast_loader', False)
        
        if use_fast_loader:
            # ä½¿ç”¨å¿«é€Ÿæ•°æ®åŠ è½½å™¨
            from merlin.training.fast_dataloader import create_fast_data_loaders
            train_loader, val_loader = create_fast_data_loaders(config)
            print(f"âœ… ä½¿ç”¨å¿«é€Ÿæ•°æ®åŠ è½½å™¨ - è®­ç»ƒé›†: {len(train_loader.dataset)}, éªŒè¯é›†: {len(val_loader.dataset)}")
        else:
            # ä½¿ç”¨æ ‡å‡†æ•°æ®åŠ è½½å™¨
            train_loader, val_loader = create_data_loaders(config)
            print(f"âœ… ä½¿ç”¨æ ‡å‡†æ•°æ®åŠ è½½å™¨ - è®­ç»ƒé›†: {len(train_loader.dataset)}, éªŒè¯é›†: {len(val_loader.dataset)}")
        
        # åˆ›å»ºè®­ç»ƒå™¨
        print("\nğŸ¤– åˆå§‹åŒ–è®­ç»ƒå™¨...")
        trainer = CardiacTrainer(config)
        
        # å¼€å§‹è®­ç»ƒ
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        trainer.train(train_loader, val_loader)
        
        # è®­ç»ƒå®Œæˆåçš„ä¿¡æ¯
        print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("=" * 80)
        print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   ğŸ† æœ€ä½³æ¨¡å‹: {config['output_dir']}/best_model.pth")
        print(f"   ğŸ“Š è®­ç»ƒæ—¥å¿—: {config['output_dir']}/training.log")
        print(f"   âš™ï¸  é…ç½®æ–‡ä»¶: {config['output_dir']}/config.json")
        print(f"   ğŸ“ˆ TensorBoard: {config['output_dir']}/tensorboard")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("   1. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—äº†è§£è¯¦ç»†ä¿¡æ¯")
        print("   2. ä½¿ç”¨TensorBoardå¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹:")
        print(f"      tensorboard --logdir {config['output_dir']}/tensorboard")
        print("   3. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹")
        print("   4. åŠ é€Ÿè®­ç»ƒæç¤º:")
        print("      - é¢„å¤„ç†æ•°æ®ä»¥åŠ é€Ÿåç»­è®­ç»ƒ:")
        print("        python -m merlin.training.data_preprocessor --config config.json")
        print("      - ä½¿ç”¨å¿«é€Ÿæ•°æ®åŠ è½½å™¨:")
        print("        python cardiac_training_example.py --use_fast_loader --preprocessed_data_dir outputs/preprocessed_data")
        print("=" * 80)
        
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        print("å·²ä¿å­˜çš„æ£€æŸ¥ç‚¹å¯ç”¨äºæ¢å¤è®­ç»ƒ")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("\nğŸ” æ•…éšœæ’é™¤å»ºè®®:")
        print("   1. æ£€æŸ¥CSVæ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("   2. æ£€æŸ¥æ•°æ®ç›®å½•è·¯å¾„æ˜¯å¦å­˜åœ¨")
        print("   3. æ£€æŸ¥GPUå†…å­˜æ˜¯å¦è¶³å¤Ÿï¼ˆå¯å°è¯•å‡å°batch_sizeï¼‰")
        print("   4. æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³")
        print("   5. æŸ¥çœ‹å®Œæ•´é”™è¯¯ä¿¡æ¯:")
        raise


if __name__ == '__main__':
    main() 